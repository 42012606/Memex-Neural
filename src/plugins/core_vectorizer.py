
import logging
from src.core.plugins import BasePlugin, EventBus
from src.core.event_types import ARCHIVE_COMPLETED, VECTORIZATION_COMPLETED
from src.core.events import Event
from src.services.ai_service import AIService
from src.core.database import SessionLocal
from src.models.archive import ArchiveRecord
from datetime import datetime

logger = logging.getLogger(__name__)

class CoreVectorizerPlugin(BasePlugin):
    """
    æ ¸å¿ƒå‘é‡åŒ–æ’ä»¶
    èŒè´£:
    1. ç›‘å¬ ARCHIVE_COMPLETED
    2. èŽ·å– full_text æˆ– summary
    3. è°ƒç”¨ AI Embedding
    4. æ›´æ–°æ•°æ®åº“
    """
    
    def __init__(self):
        self.ai = AIService()

    @property
    def name(self) -> str:
        return "CoreVectorizerPlugin"

    def register(self, bus: EventBus):
        bus.subscribe(ARCHIVE_COMPLETED, self.handle_archive_completed)
        logger.info("âœ… æ ¸å¿ƒå‘é‡åŒ–æ’ä»¶(CoreVectorizerPlugin)å·²æ³¨å†Œï¼Œæ­£åœ¨ç›‘å¬ ARCHIVE_COMPLETED äº‹ä»¶")

    async def handle_archive_completed(self, event: Event):
        archive_id = event.payload.get("archive_id")
        logger.info(f"ðŸ§© [å‘é‡åŒ–æ’ä»¶] å‡†å¤‡å‘é‡åŒ–å½’æ¡£ ID: {archive_id}")
        
        # å®žé™…é€»è¾‘å¾… CoreArchiverPlugin å¯ç”¨å¹¶å‘å°„äº‹ä»¶åŽæ‰ä¼šè§¦å‘
        await self._process_vectorization(archive_id)

    async def _process_vectorization(self, archive_id: int):
        db = SessionLocal()
        try:
            record = db.query(ArchiveRecord).filter(ArchiveRecord.id == archive_id).first()
            if not record:
                return
            
            # [Core Upgrade] èŽ·å–è¦å‘é‡åŒ–çš„æ–‡æœ¬
            # [Phase 3] Metadata Injection: Inject Title/Type/Tags into text
            tags_data = record.meta_data.get("tags", []) if isinstance(record.meta_data, dict) else []
            if isinstance(tags_data, str): 
                tags_list = [tags_data]
            elif isinstance(tags_data, list):
                tags_list = [str(t) for t in tags_data]
            else:
                tags_list = []

            meta_header = (
                f"Title: {record.filename}\n"
                f"Type: {record.file_type}\n"
                f"Category: {record.category or 'Uncategorized'}\n"
                f"Tags: {', '.join(tags_list)}\n"
                f"Summary: {record.summary or 'N/A'}\n"
                f"---\n"
            )

            # ä¼˜å…ˆä½¿ç”¨ full_text (å›¾ç‰‡ocr/æ–‡æ¡£å†…å®¹)ï¼Œå…¶æ¬¡ summaryï¼Œæœ€åŽ filename
            body_content = record.full_text or record.summary or record.filename or ""
            text_to_embed = meta_header + body_content

            if not text_to_embed.strip():
                logger.warning(f"âš ï¸ å½’æ¡£ {archive_id} æ²¡æœ‰å¯ç”¨äºŽå‘é‡åŒ–çš„æ–‡æœ¬")
                return

            logger.info(f"ðŸ§© [å‘é‡åŒ–æ’ä»¶] å¼€å§‹å¤„ç†å½’æ¡£ {archive_id}ï¼Œé•¿åº¦: {len(text_to_embed)} å­—ç¬¦")
            
            # -------------------------------------------------------------------------
            # 1. ä¼ ç»Ÿçš„ç²—ç²’åº¦å‘é‡ (Coarse-grained Vector) - ä¿æŒå…¼å®¹æ€§
            # å°†å¯¹åº”æ•´ä¸ª Archive çš„å‘é‡å­˜å…¥ archives è¡¨
            # -------------------------------------------------------------------------
            import asyncio
            
            # å¦‚æžœæ–‡æœ¬å¤ªé•¿ï¼Œæˆªæ–­ç”¨äºŽä¸»å‘é‡ (é˜²æ­¢ token æº¢å‡ºï¼ŒDashScope ä¸€èˆ¬é™åˆ¶ 2048-8000 tokens)
            # è¿™é‡Œå–å‰ 2000 å­—ç¬¦ä½œä¸º"ä¸»æ‘˜è¦"
            coarse_text = text_to_embed[:2000] 
            
            vector = await asyncio.to_thread(self.ai.embed_text, coarse_text)
            if vector:
                record.embedding = vector 
                record.is_vectorized = 1
                record.vectorized_at = datetime.now()
                logger.info(f"  Existing archive vector updated.")

            # -------------------------------------------------------------------------
            # 2. ç»†ç²’åº¦åˆ‡ç‰‡ (Fine-grained Chunking) - Parent-Child Indexing
            # å°†æ–‡æœ¬åˆ‡åˆ†ä¸ºå¤šä¸ª Child Nodesï¼Œå­˜å…¥ vector_nodes è¡¨
            # -------------------------------------------------------------------------
            from src.models.vector_node import VectorNode
            
            # æ¸…ç†æ—§çš„ vector nodes (é˜²æ­¢é‡å¤)
            # db.query(VectorNode).filter(VectorNode.parent_archive_id == archive_id).delete()
            # æš‚æ—¶ä¸åˆ é™¤ï¼Œå‡è®¾æ˜¯æ–°å¢žæˆ–è¦†ç›–ã€‚å¦‚æžœéœ€è¦å¹‚ç­‰æ€§ï¼Œåº”è¯¥å…ˆåˆ åŽåŠ ã€‚
            # ä¸ºäº†å®‰å…¨èµ·è§ï¼Œå…ˆæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing_count = db.query(VectorNode).filter(VectorNode.parent_archive_id == archive_id).count()
            if existing_count > 0:
                 logger.info(f"  Cleaning up {existing_count} existing vector nodes...")
                 db.query(VectorNode).filter(VectorNode.parent_archive_id == archive_id).delete()

            # åˆ‡ç‰‡å‚æ•°
            CHUNK_SIZE = 2000  # [Optimization] Increased from 500 to 2000 to avoid over-chunking
            OVERLAP = 200     # ä¸Šä¸‹æ–‡é‡å 
            
            chunks = []
            if len(text_to_embed) > CHUNK_SIZE:
                 start = 0
                 while start < len(text_to_embed):
                     end = min(start + CHUNK_SIZE, len(text_to_embed))
                     # å°è¯•åœ¨æ ‡ç‚¹ç¬¦å·å¤„æ–­å¼€ (ç®€å•çš„ split ä¼˜åŒ–)
                     # if end < len(text)... find last newline or punctuation...
                     # æš‚ä¸”ç®€å•åˆ‡åˆ†
                     chunk_text = text_to_embed[start:end]
                     chunks.append(chunk_text)
                     start += (CHUNK_SIZE - OVERLAP)
            else:
                 chunks.append(text_to_embed)
            
            logger.info(f"  Generated {len(chunks)} chunks. Starting batch embedding...")

            # æ‰¹é‡æˆ–å¾ªçŽ¯ç”Ÿæˆå‘é‡
            # ç›®å‰ AIService.embed_text æ˜¯å•æ¡çš„ï¼Œæˆ‘ä»¬å¾ªçŽ¯è°ƒç”¨
            # (æœªæ¥å¯ä»¥ä¼˜åŒ–ä¸º batch æŽ¥å£)
            
            created_nodes = 0
            for i, chunk in enumerate(chunks):
                # è·³è¿‡å¤ªçŸ­çš„ç¢Žç‰‡
                if len(chunk.strip()) < 10:
                    continue
                
                # ä¸ºæ¯ä¸ª chunk ç”Ÿæˆå‘é‡
                chunk_vector = await asyncio.to_thread(self.ai.embed_text, chunk)
                
                if chunk_vector:
                    node = VectorNode(
                        parent_archive_id=archive_id,
                        content=chunk,
                        chunk_index=i,
                        embedding=chunk_vector,
                        meta={"source_length": len(text_to_embed), "is_image_desc": record.file_type == "image"}
                    )
                    db.add(node)
                    created_nodes += 1
            
            db.commit()
            logger.info(f"âœ… [å‘é‡åŒ–æ’ä»¶] å½’æ¡£ {archive_id} å®Œæˆ: ä¸»å‘é‡ + {created_nodes} ä¸ª Child Nodes")

        except Exception as e:
            logger.error(f"âŒ å‘é‡åŒ–å¤±è´¥: {e}", exc_info=True)
            db.rollback()
        finally:
            db.close()
