import shutil
import logging
import os
from typing import List, Optional
from pathlib import Path
from datetime import datetime

from fastapi import (
    APIRouter,
    UploadFile,
    File,
    Depends,
    HTTPException,
    Form,
    BackgroundTasks,
    status,
    Request,
)
from fastapi.encoders import jsonable_encoder
from fastapi.responses import JSONResponse, FileResponse
from sqlalchemy.orm import Session
from pydantic import BaseModel

from src.core.database import get_db, SessionLocal
from src.core.dependencies import get_current_user
from src.core.config import settings
from src.services.processor import FileProcessor
from src.services.ai_service import AIService
from src.models.archive import ArchiveRecord, ProcessingStatus
from src.models.storage import StorageRoot
from src.models.user import User
from src.models.chat import ChatMessage
from src.models.session import ChatSession
from datetime import datetime

# åˆå§‹åŒ–
router = APIRouter()
logger = logging.getLogger(__name__)

# --- Pydantic Models (API æ•°æ®æ¨¡å‹) ---

class ArchiveResponse(BaseModel):
    id: int
    filename: str
    category: str
    summary: str
    # [New] V3.1 æ–°å¢å­—æ®µ
    confidence: Optional[int] = None
    reasoning: Optional[str] = None

class RecentRecord(BaseModel):
    id: int
    filename: str
    category: str
    status: str
    time: str

class LogResponse(BaseModel):
    logs: List[str]

class UploadAcceptedResponse(BaseModel):
    id: int
    status: str


class ArchiveDetailResponse(BaseModel):
    """å•æ¡å½’æ¡£æŸ¥è¯¢å“åº”"""

    id: int
    filename: str
    original_filename: Optional[str] = None
    category: Optional[str] = None
    subcategory: Optional[str] = None
    file_type: str
    processing_status: str
    processing_error: Optional[str] = None
    summary: Optional[str] = None
    full_text: Optional[str] = None
    path: Optional[str] = None  # å…¼å®¹å­—æ®µï¼Œè¿”å› relative_path
    storage_root_id: Optional[int] = None
    relative_path: Optional[str] = None
    file_size: Optional[int] = None
    meta_data: Optional[dict] = None
    is_vectorized: Optional[int] = None
    vectorized_at: Optional[datetime] = None
    processed_at: Optional[datetime] = None

# --- Endpoints ---

@router.post("/upload", response_model=UploadAcceptedResponse, status_code=status.HTTP_202_ACCEPTED)
async def upload_file(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    model_id: Optional[str] = Form(None),
    session_id: Optional[str] = Form(None),  # [Persistence]
    current_user_id: int = Depends(get_current_user),
    db: Session = Depends(get_db),
):
    """
    [Core] ä¸Šä¼ æ–‡ä»¶ -> ç«‹å³å…¥åº“ pending -> èƒŒæ™¯ä»»åŠ¡å¼‚æ­¥å¤„ç†
    """
    try:
        # [Persistence] Record upload in chat history
        if session_id:
            try:
                # User message: file upload notification
                user_msg = ChatMessage(
                    role="user",
                    content=f"ğŸ“„ [File Upload] {file.filename}", 
                    model_id="system_upload",
                    session_id=session_id,
                    user_id=current_user_id
                )
                db.add(user_msg)
                db.commit()
                # [NOTE] å®Œæˆæ¶ˆæ¯å°†åœ¨å½’æ¡£å¤„ç†æˆåŠŸ/å¤±è´¥åç”±æ’ä»¶ä¿å­˜
            except Exception as msg_err:
                logger.warning(f"Failed to persist upload message: {msg_err}")

        processor = FileProcessor()
        file_size = 0
        file_type = None
        user = db.query(User).filter(User.id == current_user_id).first()
        username = processor._sanitize_username(user.username if user else f"user_{current_user_id}")
        storage_root = processor._get_default_storage_root(db)

        # ä½¿ç”¨å­˜å‚¨å· + ç”¨æˆ·åçš„ INBOX ç›®å½•
        inbox_path = Path(storage_root.mount_path) / username / "_INBOX"
        inbox_path.mkdir(parents=True, exist_ok=True)
        temp_path = inbox_path / file.filename

        with open(temp_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        file_size = temp_path.stat().st_size if temp_path.exists() else 0
        file_type = processor._get_file_type(temp_path)

        record = ArchiveRecord(
            user_id=current_user_id,
            filename=file.filename,
            original_filename=file.filename,
            file_type=file_type or "Documents",
            category="æœªåˆ†ç±»",
            subcategory="",
            summary="",
            full_text=None,
            storage_root_id=storage_root.id,
            relative_path=str(temp_path.relative_to(storage_root.mount_path).as_posix()),
            file_size=file_size,
            processing_status=ProcessingStatus.PENDING.value,
            processing_error=None,
            processed_at=datetime.now(),
            meta_data={
                "original_filename": file.filename,
                "file_size": file_size,
                "uploaded_at": datetime.utcnow().isoformat(),
                "session_id": session_id,  # [NEW] ç”¨äºå½’æ¡£å®Œæˆåä¿å­˜èŠå¤©è®°å½•
            },
        )

        db.add(record)
        db.commit()
        db.refresh(record)

        background_tasks.add_task(
            processor.process_file_background,
            str(temp_path),
            record.id,
            model_id,
        )

        return JSONResponse(
            status_code=status.HTTP_202_ACCEPTED,
            content={"id": record.id, "status": ProcessingStatus.PENDING.value},
        )
    except Exception as e:
        logger.error(f"Upload failed: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"å¤„ç†å¤±è´¥: {str(e)}")


@router.get("/archives/{archive_id}", response_model=ArchiveDetailResponse)
async def get_archive_detail(
    archive_id: int,
    current_user_id: int = Depends(get_current_user),
    db: Session = Depends(get_db),
):
    """å•æ¡å½’æ¡£æŸ¥è¯¢ï¼šæä¾›ç²¾ç¡®å¤„ç†çŠ¶æ€å’Œå…ƒæ•°æ®"""
    try:
        record = (
            db.query(ArchiveRecord)
            .filter(ArchiveRecord.id == archive_id, ArchiveRecord.user_id == current_user_id)
            .first()
        )
        if not record:
            raise HTTPException(status_code=404, detail="Archive not found")

        payload = {
            "id": record.id,
            "filename": record.filename,
            "original_filename": record.original_filename,
            "category": record.category,
            "subcategory": record.subcategory,
            "file_type": record.file_type,
            "processing_status": record.processing_status,
            "processing_error": record.processing_error,
            "summary": record.summary,
            "full_text": record.full_text,
            "path": record.relative_path,  # å…¼å®¹æ—§å­—æ®µ
            "storage_root_id": record.storage_root_id,
            "relative_path": record.relative_path,
            "file_size": record.file_size,
            "meta_data": record.meta_data or {},
            "is_vectorized": record.is_vectorized,
            "vectorized_at": record.vectorized_at,
            "processed_at": record.processed_at,
        }

        status_code = status.HTTP_200_OK
        if record.processing_status in (
            ProcessingStatus.PENDING.value,
            ProcessingStatus.PROCESSING.value,
        ):
            status_code = status.HTTP_202_ACCEPTED

        return JSONResponse(status_code=status_code, content=jsonable_encoder(payload))
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–å½’æ¡£è¯¦æƒ…å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="è·å–å½’æ¡£è¯¦æƒ…å¤±è´¥")


@router.get("/archives", response_model=List[ArchiveResponse])
async def get_all_archives(
    skip: int = 0,
    limit: int = 100,
    current_user_id: int = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """[Knowledge Base] è·å–å½’æ¡£åˆ—è¡¨"""
    records = (
        db.query(ArchiveRecord)
        .filter(ArchiveRecord.user_id == current_user_id)
        .order_by(ArchiveRecord.id.desc())
        .offset(skip)
        .limit(limit)
        .all()
    )
    results = []
    for r in records:
        results.append({
            "id": r.id,
            "filename": r.filename,
            "category": f"{r.category}/{r.subcategory}" if r.subcategory else r.category,
            "summary": r.summary or "æ— æ‘˜è¦",
            "confidence": 100 if r.processing_status == "completed" else 0, # Placeholder
            "reasoning": r.processing_error
        })
    return results


@router.get("/recents", response_model=List[RecentRecord])
async def get_recent_records(
    limit: int = 10,
    current_user_id: int = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """[Utility] è·å–æœ€è¿‘å½’æ¡£åˆ—è¡¨"""
    records = (
        db.query(ArchiveRecord)
        .filter(ArchiveRecord.user_id == current_user_id)
        .order_by(ArchiveRecord.id.desc())
        .limit(limit)
        .all()
    )
    results = []
    for r in records:
        results.append({
            "id": r.id,
            "filename": r.filename,
            "category": f"{r.category}/{r.subcategory}",
            "status": "âœ… å·²å½’æ¡£",
            "time": r.processed_at.strftime("%m-%d %H:%M")
        })







@router.get("/logs", response_model=LogResponse)
async def get_system_logs(lines: int = 50):
    """
    [Utility] ä»å†…å­˜è¯»å–å®æ—¶æ—¥å¿— (é¿å…æ–‡ä»¶é”é˜»å¡)
    """
    from src.core.log_manager import log_manager
    try:
        # ä»å†…å­˜è·å–æ—¥å¿—
        raw_logs = log_manager.get_logs()
        # å–æœ€å lines è¡Œ
        tail_lines = raw_logs[-lines:] if lines > 0 else raw_logs
        return {"logs": tail_lines}
    except Exception as e:
        return {"logs": [f"âŒ è¯»å–å†…å­˜æ—¥å¿—å¤±è´¥: {str(e)}"]}


@router.get("/files/{file_path:path}")
async def serve_file(
    file_path: str,
    request: Request,
    current_user_id: int = Depends(get_current_user)
):
    """
    [File Service] æä¾›æ–‡ä»¶è®¿é—®æœåŠ¡ï¼Œç”¨äº DashScope API è®¿é—®æœ¬åœ°æ–‡ä»¶
    æ”¯æŒéŸ³é¢‘å’Œå›¾ç‰‡æ–‡ä»¶çš„ HTTP è®¿é—®
    """
    try:
        processor = FileProcessor()
        db = SessionLocal()
        try:
            storage_root = processor._get_default_storage_root(db)
            base_dir = Path(storage_root.mount_path)
            full_path = base_dir / file_path
        finally:
            db.close()
        
        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æ–‡ä»¶åœ¨ç”¨æˆ·æ•°æ®ç›®å½•å†…
        try:
            full_path.resolve().relative_to(base_dir.resolve())
        except ValueError:
            raise HTTPException(status_code=403, detail="æ–‡ä»¶è·¯å¾„ä¸åœ¨å…è®¸çš„ç›®å½•å†…")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not full_path.exists():
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        if not full_path.is_file():
            raise HTTPException(status_code=400, detail="è·¯å¾„ä¸æ˜¯æ–‡ä»¶")
        
        # è¿”å›æ–‡ä»¶ï¼Œé’ˆå¯¹éƒ¨åˆ†æ ¼å¼å¼ºåˆ¶æ­£ç¡®çš„ Content-Type
        media_type = "application/octet-stream"
        suffix = full_path.suffix.lower()
        if suffix == ".m4a":
            media_type = "audio/mp4"
        elif suffix == ".mp3":
            media_type = "audio/mpeg"
        elif suffix == ".wav":
            media_type = "audio/wav"
        elif suffix == ".flac":
            media_type = "audio/flac"
        elif suffix == ".ogg":
            media_type = "audio/ogg"

        return FileResponse(
            path=str(full_path),
            filename=full_path.name,
            media_type=media_type
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æä¾›æ–‡ä»¶è®¿é—®å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶è®¿é—®å¤±è´¥: {str(e)}")