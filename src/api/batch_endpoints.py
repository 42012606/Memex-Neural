# src/api/batch_endpoints.py
"""
æ‰¹é‡å¯¼å…¥ API ç«¯ç‚¹
"""
import logging
import time
from pathlib import Path
from typing import List, Optional
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from sqlalchemy.orm import Session
from pydantic import BaseModel
from src.core.database import get_db, SessionLocal
from src.core.dependencies import get_current_user
from src.core.config import settings
from src.services.processor import FileProcessor

router = APIRouter()
logger = logging.getLogger(__name__)

# å…¨å±€ä»»åŠ¡çŠ¶æ€ï¼ˆç®€å•å®ç°ï¼Œç”Ÿäº§ç¯å¢ƒå»ºè®®ç”¨ Redis æˆ–æ•°æ®åº“ï¼‰
batch_tasks = {}


class BatchImportRequest(BaseModel):
    """æ‰¹é‡å¯¼å…¥è¯·æ±‚"""
    file_paths: List[str]  # æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆç›¸å¯¹è·¯å¾„æˆ–ç»å¯¹è·¯å¾„ï¼‰
    model_id: Optional[str] = "gemini-2.5-flash"
    rate_limit: Optional[float] = 0.5  # æ¯ä¸ªæ–‡ä»¶å¤„ç†é—´éš”ï¼ˆç§’ï¼‰


class BatchImportResponse(BaseModel):
    """æ‰¹é‡å¯¼å…¥å“åº”"""
    task_id: str
    total_files: int
    message: str


class BatchStatusResponse(BaseModel):
    """æ‰¹é‡å¯¼å…¥çŠ¶æ€å“åº”"""
    task_id: str
    status: str  # pending/processing/completed/failed
    total: int
    processed: int
    succeeded: int
    failed: int
    current_file: Optional[str] = None
    errors: List[str] = []


def process_batch_files(
    task_id: str,
    file_paths: List[str],
    model_id: str,
    rate_limit: float,
    user_id: int
):
    """
    åå°å¤„ç†æ‰¹é‡æ–‡ä»¶ï¼ˆåœ¨åå°ä»»åŠ¡ä¸­æ‰§è¡Œï¼‰
    """
    from src.models.archive import ArchiveRecord, ProcessingStatus
    from datetime import datetime

    task_info = batch_tasks[task_id]
    task_info["status"] = "processing"
    processor = FileProcessor()
    
    succeeded = 0
    failed = 0
    errors = []
    
    # è·å–æ•°æ®åº“ä¼šè¯ç”¨äºåˆ›å»ºåˆå§‹è®°å½•
    db = SessionLocal()
    
    try:
        for idx, file_path_str in enumerate(file_paths):
            file_path = Path(file_path_str)
            
            # æ›´æ–°å½“å‰å¤„ç†æ–‡ä»¶ï¼ˆè°ƒåº¦ä¸­ï¼‰
            task_info["current_file"] = file_path.name
            task_info["processed"] = idx + 1
            
            if not file_path.exists():
                failed += 1
                error_msg = f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
                errors.append(error_msg)
                logger.warning(f"âš ï¸ [{task_id}] {error_msg}")
                continue
            
            try:
                # 1. å‡†å¤‡åŸºç¡€ä¿¡æ¯
                file_size = file_path.stat().st_size
                file_type = processor._get_file_type(file_path)
                
                # [Dynamic Storage Root Detection]
                #Iterate through all active roots to find the correct parent
                from src.models.storage import StorageRoot
                active_roots = db.query(StorageRoot).filter(StorageRoot.is_active.is_(True)).all()
                
                target_root = None
                relative_path = file_path.name # Default fallback
                
                # Explicitly check relative_to for each root
                for root in active_roots:
                    try:
                        # Resolve paths to ensure consistent comparison
                        root_path = Path(root.mount_path).resolve()
                        target_file_path = file_path.resolve()
                        
                        if target_file_path.is_relative_to(root_path):
                            target_root = root
                            relative_path = str(target_file_path.relative_to(root_path).as_posix())
                            break
                    except Exception:
                        continue
                
                # Fallback to default root if no parent match found (e.g. file outside known roots)
                if not target_root:
                     target_root = next((r for r in active_roots if r.is_default), active_roots[0] if active_roots else None)
                     # Keep relative_path as filename since we can't calculate a real relative path
                
                if not target_root:
                    raise RuntimeError("No active storage roots configured")

                record = ArchiveRecord(
                    user_id=user_id,
                    filename=file_path.name,
                    original_filename=file_path.name,
                    file_type=file_type or "Documents",
                    category="æœªåˆ†ç±»",
                    subcategory="",
                    summary="",
                    full_text=None,
                    storage_root_id=target_root.id,
                    relative_path=relative_path,
                    file_size=file_size,
                    processing_status=ProcessingStatus.PENDING.value,
                    processing_error=None,
                    processed_at=datetime.now(),
                    meta_data={
                        "original_filename": file_path.name,
                        "file_size": file_size,
                        "batch_task_id": task_id,
                        "imported_at": datetime.utcnow().isoformat(),
                    },
                )
                
                db.add(record)
                db.commit()
                db.refresh(record)
                
                # 3. è§¦å‘åå°å¤„ç†ï¼ˆå‘é€äº‹ä»¶ï¼‰
                # 3. è§¦å‘åå°å¤„ç†ï¼ˆå‘é€äº‹ä»¶ï¼‰
                # process_file_background è´Ÿè´£å‘å°„ FILE_UPLOADED äº‹ä»¶
                is_success = processor.process_file_background(
                    str(file_path),
                    record.id,
                    model_id
                )
                
                if is_success:
                    succeeded += 1
                    logger.info(f"âœ… [{task_id}] å¤„ç†æˆåŠŸ: {file_path.name}")
                else:
                    failed += 1
                    err_msg = f"å¤„ç†å¤±è´¥ (Internal Error): {file_path.name}"
                    errors.append(err_msg)
                    logger.error(f"âŒ [{task_id}] {err_msg}")
                
            except Exception as e:
                failed += 1
                error_msg = f"è°ƒåº¦å¤±è´¥ {file_path.name}: {str(e)}"
                errors.append(error_msg)
                logger.error(f"âŒ [{task_id}] {error_msg}")
                # å°è¯•å›æ»š
                db.rollback()
            
            # ç®€å•é™é€Ÿï¼Œé˜²æ­¢ç¬é—´å‘å°„è¿‡å¤šäº‹ä»¶
            if idx < len(file_paths) - 1:
                time.sleep(0.1) 

    finally:
        db.close()
    
    # æ›´æ–°ä»»åŠ¡çŠ¶æ€
    task_info["status"] = "completed"
    task_info["succeeded"] = succeeded
    task_info["failed"] = failed
    task_info["errors"] = errors
    task_info["current_file"] = None
    logger.info(f"âœ… [{task_id}] æ‰¹é‡å¯¼å…¥è°ƒåº¦å®Œæˆ: æˆåŠŸ {succeeded}, å¤±è´¥ {failed}")


@router.post("/batch/import", response_model=BatchImportResponse)
async def batch_import(
    request: BatchImportRequest,
    background_tasks: BackgroundTasks,
    current_user_id: int = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    æ‰¹é‡å¯¼å…¥æ–‡ä»¶
    """
    import uuid
    
    # ç”Ÿæˆä»»åŠ¡ ID
    task_id = str(uuid.uuid4())
    
    # éªŒè¯æ–‡ä»¶è·¯å¾„
    valid_paths = []
    for path_str in request.file_paths:
        path = Path(path_str)
        if path.exists() and path.is_file():
            valid_paths.append(str(path.absolute()))
        else:
            logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸æ˜¯æ–‡ä»¶: {path_str}")
    
    if not valid_paths:
        raise HTTPException(status_code=400, detail="æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶è·¯å¾„")
    
    # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
    batch_tasks[task_id] = {
        "status": "pending",
        "total": len(valid_paths),
        "processed": 0,
        "succeeded": 0,
        "failed": 0,
        "current_file": None,
        "errors": []
    }
    
    # æ·»åŠ åˆ°åå°ä»»åŠ¡
    background_tasks.add_task(
        process_batch_files,
        task_id,
        valid_paths,
        request.model_id,
        request.rate_limit,
        current_user_id
    )
    
    logger.info(f"ğŸ“¦ æ‰¹é‡å¯¼å…¥ä»»åŠ¡å·²åˆ›å»º: {task_id}, æ–‡ä»¶æ•°: {len(valid_paths)}")
    
    return {
        "task_id": task_id,
        "total_files": len(valid_paths),
        "message": f"æ‰¹é‡å¯¼å…¥ä»»åŠ¡å·²åˆ›å»ºï¼Œå…± {len(valid_paths)} ä¸ªæ–‡ä»¶"
    }


@router.get("/batch/status/{task_id}", response_model=BatchStatusResponse)
async def get_batch_status(task_id: str):
    """
    è·å–æ‰¹é‡å¯¼å…¥ä»»åŠ¡çŠ¶æ€
    """
    if task_id not in batch_tasks:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    task_info = batch_tasks[task_id]
    
    return {
        "task_id": task_id,
        "status": task_info["status"],
        "total": task_info["total"],
        "processed": task_info["processed"],
        "succeeded": task_info["succeeded"],
        "failed": task_info["failed"],
        "current_file": task_info.get("current_file"),
        "errors": task_info.get("errors", [])[:10]  # åªè¿”å›å‰10ä¸ªé”™è¯¯
    }