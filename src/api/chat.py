import logging
import re
import uuid
from typing import List, Optional
from datetime import datetime
from fastapi import APIRouter, Depends, HTTPException, Body, UploadFile, File, Form
import shutil
import os
import base64
from pathlib import Path
from starlette.concurrency import run_in_threadpool
from sqlalchemy.orm import Session
from sqlalchemy import desc, or_, nullslast
from pydantic import BaseModel

from src.core.database import get_db
from src.core.dependencies import get_current_user
from src.models.chat import ChatMessage
from src.models.session import ChatSession
from src.services.ai_service import AIService
from src.services.agents.router_agent import RouterAgent
from src.services.agents.retrieval_agent import RetrievalAgent
from src.services.context_memory import ContextMemoryService
from src.models.archive import ArchiveRecord
from src.services.file_service import get_file_public_url
from src.services.export_service import ExportService
from src.utils.text_tools import estimate_token_count

router = APIRouter()
logger = logging.getLogger(__name__)


def _find_file_ids_by_terms(db: Session, user_id: int, terms: List[str], limit: int = 3) -> List[int]:
    """
    å°è¯•é€šè¿‡å…³é”®è¯/æ–‡ä»¶åç‰‡æ®µåŒ¹é…æ¡£æ¡ˆ IDã€‚
    é€‚ç”¨äºâ€œåˆšåˆšä¸Šä¼ çš„æ–‡ä»¶â€â€œ20231115_ä½“æ£€æŠ¥å‘Š.txtâ€ç­‰å¼±æŒ‡ä»£åœºæ™¯ã€‚
    """
    ids: List[int] = []
    seen = set()
    for term in terms or []:
        if not term:
            continue
        cleaned = term.strip()
        if not cleaned:
            continue
        try:
            candidates = (
                db.query(ArchiveRecord)
                .filter(
                    ArchiveRecord.user_id == user_id,
                    or_(
                        ArchiveRecord.filename.ilike(f"%{cleaned}%"),
                        ArchiveRecord.original_filename.ilike(f"%{cleaned}%"),
                    ),
                )
                .order_by(ArchiveRecord.processed_at.desc())
                .limit(limit)
                .all()
            )
            for c in candidates:
                if c.id not in seen:
                    ids.append(c.id)
                    seen.add(c.id)
        except Exception as e:
            logger.warning(f"åŒ¹é…æ–‡ä»¶åç‰‡æ®µå¤±è´¥: term={cleaned}, error={e}")
    return ids[:limit]


def _looks_like_file_reference(text: str) -> bool:
    """
    ç®€å•åˆ¤æ–­å­—ç¬¦ä¸²æ˜¯å¦åƒæ–‡ä»¶å¼•ç”¨ï¼ˆå‡å°‘ç¡¬ç¼–ç è¯è¡¨ä¾èµ–ï¼‰ã€‚
    è§„åˆ™ï¼š
    - å«å¸¸è§æ‰©å±•å
    - æˆ–åŒ¹é… YYYYMMDD_*.xxx/æŠ¥å‘Š/æ–‡ä»¶ ä¹‹ç±»çš„æ¨¡å¼
    """
    if not text:
        return False
    t = text.strip()
    if len(t) < 3:
        return False
    lowered = t.lower()
    # å¸¸è§æ‰©å±•
    for ext in [".txt", ".pdf", ".doc", ".docx", ".md", ".ppt", ".pptx", ".xls", ".xlsx"]:
        if ext in lowered:
            return True
    # å½¢å¦‚ 20231115_ä½“æ£€æŠ¥å‘Š.txt æˆ– 20231115_*
    import re as _re
    if _re.search(r"\b20\d{6}[_-]?\S*", t):
        return True
    # å‡ºç°â€œæ–‡ä»¶â€â€œæŠ¥å‘Šâ€ä¸”æœ‰æ•°å­—æˆ–ä¸‹åˆ’çº¿
    if ("æ–‡ä»¶" in t or "æŠ¥å‘Š" in t) and any(ch.isdigit() for ch in t):
        return True
    return False

# --- Pydantic Models ---
class CreateSessionRequest(BaseModel):
    title: Optional[str] = None

class RenameSessionRequest(BaseModel):
    title: str

class ChatSessionResponse(BaseModel):
    id: str  # UUID
    title: str
    created_at: datetime
    updated_at: datetime
    user_id: int

class ChatRequest(BaseModel):
    query: str
    model_id: Optional[str] = None
    session_id: Optional[str] = None  # UUID

class ChatResponse(BaseModel):
    reply: str
    session_id: str
    model_id: str

class MessageResponse(BaseModel):
    role: str
    content: str
    created_at: datetime
    model_id: Optional[str] = None

# --- Session Endpoints ---

@router.get("/sessions", response_model=List[ChatSessionResponse])
async def list_sessions(
    limit: int = 20, 
    current_user_id: int = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """è·å–ä¼šè¯åˆ—è¡¨ (æŒ‰æ›´æ–°æ—¶é—´å€’åº)"""
    try:
        # æŒ‰æ›´æ–°æ—¶é—´å€’åºï¼Œå¤„ç†å¯èƒ½çš„ None å€¼
        sessions = db.query(ChatSession)\
            .filter(ChatSession.user_id == current_user_id)\
            .order_by(nullslast(desc(ChatSession.updated_at)))\
            .limit(limit)\
            .all()
        # å…¼å®¹æ—§æ•°æ®æ•´å‹IDï¼Œç»Ÿä¸€è½¬å­—ç¬¦ä¸²ï¼Œé¿å…å“åº”æ ¡éªŒå¤±è´¥
        result = []
        for s in sessions:
            try:
                result.append(ChatSessionResponse(
                    id=str(s.id) if s.id else str(uuid.uuid4()),
                    title=s.title or "",
                    created_at=s.created_at if s.created_at else datetime.now(),
                    updated_at=s.updated_at if s.updated_at else datetime.now(),
                    user_id=s.user_id if s.user_id else current_user_id,
                ))
            except Exception as session_error:
                logger.error(f"å¤„ç†ä¼šè¯ {s.id if hasattr(s, 'id') else 'unknown'} æ—¶å‡ºé”™: {session_error}", exc_info=True)
                continue
        return result
    except Exception as e:
        logger.error(f"è·å–ä¼šè¯åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"è·å–ä¼šè¯åˆ—è¡¨å¤±è´¥: {str(e)}")

@router.post("/sessions", response_model=ChatSessionResponse)
async def create_session(
    request: CreateSessionRequest = Body(default=CreateSessionRequest()),
    current_user_id: int = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """åˆ›å»ºä¸€ä¸ªæ–°ä¼šè¯ (è¿”å› UUID)"""
    try:
        # 1. è‡ªåŠ¨ç”Ÿæˆæ ‡é¢˜ (å¦‚æœæœªæä¾›)
        initial_title = request.title or "New Chat"
        
        new_session = ChatSession(
            title=initial_title,
            user_id=current_user_id
            # id is auto-generated in model via uuid.uuid4()
        )
        db.add(new_session)
        db.commit()
        db.refresh(new_session)
        
        # ç¡®ä¿è¿”å›æ­£ç¡®çš„æ ¼å¼
        session_id = str(new_session.id) if new_session.id else str(uuid.uuid4())
        return ChatSessionResponse(
            id=session_id,
            title=new_session.title or "",
            created_at=new_session.created_at if new_session.created_at else datetime.now(),
            updated_at=new_session.updated_at if new_session.updated_at else datetime.now(),
            user_id=new_session.user_id if new_session.user_id else current_user_id
        )
    except Exception as e:
        logger.error(f"åˆ›å»ºä¼šè¯å¤±è´¥: {e}", exc_info=True)
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºä¼šè¯å¤±è´¥: {str(e)}")

@router.put("/sessions/{session_id}", response_model=ChatSessionResponse)
async def rename_session(
    session_id: str,
    request: RenameSessionRequest,
    current_user_id: int = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """é‡å‘½åä¼šè¯"""
    session = db.query(ChatSession).filter(ChatSession.id == session_id, ChatSession.user_id == current_user_id).first()
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")
    
    session.title = request.title
    session.updated_at = datetime.now()
    db.commit()
    db.refresh(session)
    return session

@router.delete("/sessions/{session_id}")
async def delete_session(
    session_id: str,
    current_user_id: int = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """åˆ é™¤ä¼šè¯åŠå…¶æ‰€æœ‰æ¶ˆæ¯ (Cascade delete logic typically handled by DB or explicit deletion)"""
    session = db.query(ChatSession).filter(ChatSession.id == session_id, ChatSession.user_id == current_user_id).first()
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")
    
    # æ‰‹åŠ¨åˆ é™¤å…³è”æ¶ˆæ¯ (å› ä¸ºæ¨¡å‹é‡Œæ²¡æœ‰é…ç½® cascade delete å¼ºåˆ¶)
    db.query(ChatMessage).filter(ChatMessage.session_id == session_id).delete()
    
    db.delete(session)
    db.commit()
    return {"status": "ok", "message": "Session deleted"}

# --- Chat Endpoints ---

@router.post("/chat", response_model=ChatResponse)
async def chat_with_memex(
    request: ChatRequest,
    current_user_id: int = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """å‘é€æ¶ˆæ¯å¹¶è·å–å›å¤ (Server-Side Persistence)"""
    # Step 1: å˜é‡å®‰å…¨åˆå§‹åŒ– - åœ¨ try å—å¤–éƒ¨åˆå§‹åŒ–æ‰€æœ‰åç»­ç”¨åˆ°çš„å˜é‡
    fallback_session_id = None
    current_intent = "chat"
    file_ids = []
    router_keywords = []
    top_k = 3
    time_range = None
    intent = {"intent": "chat", "search_params": {}}
    context_text = ""
    
    try:
        session_id = request.session_id
        
        # 1. ä¼šè¯ç®¡ç†
        if not session_id:
            new_session = ChatSession(title=request.query[:30], user_id=current_user_id)
            db.add(new_session)
            db.commit()
            db.refresh(new_session)
            session_id = new_session.id
        else:
            session = db.query(ChatSession).filter(ChatSession.id == session_id).first()
            if not session:
                new_session = ChatSession(title=request.query[:30], user_id=current_user_id)
                db.add(new_session)
                db.commit()
                db.refresh(new_session)
                session_id = new_session.id
            else:
                session.updated_at = datetime.now()
                db.commit()

        fallback_session_id = session_id

        # 2. ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
        user_msg = ChatMessage(
            role="user", 
            content=request.query, 
            model_id=request.model_id, 
            session_id=session_id,
            user_id=current_user_id
        )
        db.add(user_msg)
        db.commit()
        
        # 3. æ„å»ºä¸Šä¸‹æ–‡
        ai_service = AIService()
        now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # Step 3.1: å…ˆè·å–å†å²æ‘˜è¦ï¼ˆç”¨äº Neural Routerï¼‰
        # Step 3.1: å…ˆè·å–å†å²æ‘˜è¦ï¼ˆç”¨äº Neural Routerï¼‰
        memory_service = ContextMemoryService(db=db)
        history_summary_for_router = ""
        recent_messages_list = []
        try:
            # è·å–å†å²æ¶ˆæ¯ç”¨äºæ‘˜è¦ï¼ˆæ’é™¤å½“å‰æ¶ˆæ¯ï¼‰
            all_messages = memory_service.get_recent_messages(
                session_id=session_id,
                limit=50, # Limit to 50 for efficiency
                exclude_last=1
            )
            
            # å‡†å¤‡æœ€è¿‘å¯¹è¯åˆ—è¡¨ (Dict format) ä¼ ç»™ Router
            # å–æœ€è¿‘ 10 æ¡è¶³å¤Ÿäº†ï¼ŒRouter å†…éƒ¨ä¼šæˆªå–
            for msg in all_messages[-10:]:
                recent_messages_list.append({
                    "role": msg.role,
                    "content": msg.content
                })

            # å¦‚æœæ¶ˆæ¯å¾ˆå¤šï¼Œç”Ÿæˆæ‘˜è¦ï¼›å¦åˆ™ä½¿ç”¨ç®€å•çš„å†å²æ–‡æœ¬
            if len(all_messages) >= memory_service.SUMMARY_TRIGGER_THRESHOLD:
                older_messages = all_messages[:-memory_service.SLIDING_WINDOW_SIZE]
                if older_messages:
                    history_summary_for_router = await memory_service.generate_rolling_summary(
                        older_messages, 
                        now_str
                    )
        except Exception as e:
            logger.warning(f"è·å–å†å²æ‘˜è¦å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨ç©ºæ‘˜è¦: {e}")

        # Step 3.2: Neural Router - ä½¿ç”¨å¢å¼ºç‰ˆè·¯ç”±ï¼ˆä¼ å…¥å†å²æ‘˜è¦ + æœ€è¿‘åŸæ–‡ï¼‰
        router_agent = RouterAgent()
        neural_result = None
        memory_distillation = ""
        
        try:
            neural_result = await router_agent.neural_route(
                current_input=request.query,
                history_summary=history_summary_for_router if history_summary_for_router else None,
                recent_messages=recent_messages_list
            )
            memory_distillation = neural_result.get("memory_distillation", "")
            logger.info(f"ğŸ§  Neural Router è·¯ç”±å®Œæˆ: {neural_result}")

            # [New] Handle Ambiguity / Hesitation
            if neural_result.get("intent") == "ambiguous":
                question = neural_result.get("clarification_question") or "Could you clarify what you mean?"
                
                # Save clarification as AI message
                ai_msg = ChatMessage(
                    role="assistant",
                    content=f"ğŸ¤” {question}",  # Add emoji to indicate thinking/hesitation
                    model_id="router_hesitation",
                    session_id=session_id,
                    user_id=current_user_id
                )
                db.add(ai_msg)
                db.commit()
                
                return {
                    "reply": f"ğŸ¤” {question}",
                    "session_id": session_id,
                    "model_id": "router_hesitation"
                }
        except Exception as router_error:
            logger.error(f"Neural Router Error: {router_error}", exc_info=True)
            # é™çº§åˆ°æ—§çš„ parse_intent æ–¹æ³•
            try:
                logger.info("é™çº§åˆ°æ—§ç‰ˆ Router (parse_intent)")
                intent = await router_agent.parse_intent(request.query)
                neural_result = None  # æ ‡è®°ä½¿ç”¨æ—§ç‰ˆè·¯ç”±
            except Exception as fallback_error:
                logger.error(f"æ‰€æœ‰è·¯ç”±æ¨¡å‹å‡ä¸å¯ç”¨: {fallback_error}", exc_info=True)
                error_detail = str(fallback_error)
                error_message = f"ğŸš« ç³»ç»Ÿé”™è¯¯: æ‰€æœ‰è·¯ç”±æ¨¡å‹å‡ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ API Key é…ç½®ã€‚\n\né”™è¯¯è¯¦æƒ…: {error_detail}"
                
                ai_msg = ChatMessage(
                    role="assistant",
                    content=error_message,
                    model_id=request.model_id or "system_error",
                    session_id=session_id,
                    user_id=current_user_id
                )
                db.add(ai_msg)
                db.commit()
                
                return {
                    "reply": error_message,
                    "session_id": session_id,
                    "model_id": request.model_id or "system_error"
                }
        
        # Step 3.3: è§£æè·¯ç”±ç»“æœï¼ˆæ”¯æŒæ–°æ—§ä¸¤ç§æ ¼å¼ï¼‰
        router_filters = {}
        router_file_type = None

        if neural_result:
            # å…¼å®¹æ–°ç‰ˆæ‰å¹³ç»“æ„ä¸æ—§ç‰ˆåµŒå¥—ç»“æ„
            if "needs_search" in neural_result or "keywords" in neural_result:
                needs_search = neural_result.get("needs_search", False)
                confidence = neural_result.get("confidence", 0.5)
                router_keywords = neural_result.get("keywords", []) or []
                router_filters = neural_result.get("filters") or {}
                intent_hint = "search" if needs_search else "chat"
                file_ids = []
                
                # å¦‚æœå…³é”®è¯ä¸ºç©ºä¸”éœ€è¦æœç´¢ï¼Œä»æŸ¥è¯¢æ–‡æœ¬ä¸­æå–å…³é”®è¯ï¼ˆç‰¹åˆ«æ˜¯ä¸­æ–‡ï¼‰
                if not router_keywords and needs_search:
                    # æå–ä¸­æ–‡å…³é”®è¯ï¼ˆ2-4å­—ï¼‰
                    chinese_keywords = re.findall(r'[\u4e00-\u9fff]{2,4}', request.query)
                    # æå–è‹±æ–‡å•è¯
                    english_keywords = re.findall(r'\b[a-zA-Z]{3,}\b', request.query)
                    router_keywords = chinese_keywords[:3] + english_keywords[:2]
                    if router_keywords:
                        logger.info(f"ğŸ”§ è·¯ç”±æ¨¡å‹æœªæå–å…³é”®è¯ï¼Œè‡ªåŠ¨æå–: {router_keywords}")
            else:
                routing = neural_result.get("routing", {})
                needs_search = routing.get("needs_search", False)
                confidence = routing.get("confidence_score", 0.5)
                route_target = routing.get("route_target", "direct_llm")
                if confidence < 0.8:
                    needs_search = True
                    route_target = "search_engine"
                search_payload = neural_result.get("search_payload", {})
                primary_keys = search_payload.get("primary_keys", [])
                synonym_keys = search_payload.get("synonym_keys", [])
                contextual_keys = search_payload.get("contextual_keys", [])
                intent_hint = search_payload.get("intent_hint", "search")
                router_keywords = list(set(primary_keys + synonym_keys + contextual_keys))  # å»é‡
                router_filters = search_payload.get("filters") or {}
                
                # å¦‚æœå…³é”®è¯ä¸ºç©ºä¸”éœ€è¦æœç´¢ï¼Œä»æŸ¥è¯¢æ–‡æœ¬ä¸­æå–å…³é”®è¯ï¼ˆç‰¹åˆ«æ˜¯ä¸­æ–‡ï¼‰
                if not router_keywords and needs_search:
                    # æå–ä¸­æ–‡å…³é”®è¯ï¼ˆ2-4å­—ï¼‰
                    chinese_keywords = re.findall(r'[\u4e00-\u9fff]{2,4}', request.query)
                    # æå–è‹±æ–‡å•è¯
                    english_keywords = re.findall(r'\b[a-zA-Z]{3,}\b', request.query)
                    router_keywords = chinese_keywords[:3] + english_keywords[:2]
                    if router_keywords:
                        logger.info(f"ğŸ”§ è·¯ç”±æ¨¡å‹æœªæå–å…³é”®è¯ï¼Œè‡ªåŠ¨æå–: {router_keywords}")

                file_ids = []
                for key in primary_keys:
                    id_match = re.search(r'id[:\s]*(\d+)', key, re.IGNORECASE)
                    if id_match:
                        try:
                            file_ids.append(int(id_match.group(1)))
                        except:
                            pass

            if intent_hint == "analyze":
                current_intent = "analyze"
                need_full_context = True
                router_keywords = []
                file_ids = []
            elif intent_hint == "file_read":
                current_intent = "file_read"
                need_full_context = True
            elif intent_hint == "chat":
                current_intent = "chat"
                need_full_context = False
                needs_search = False
            else:
                current_intent = "search"
                need_full_context = False

            # å¦‚æœç”¨æˆ·è¯´â€œåˆšåˆš/æœ€æ–°ä¸Šä¼ çš„æ–‡ä»¶â€ä½†è·¯ç”±åˆ¤ä¸ºæœç´¢ï¼Œå¼ºåˆ¶åˆ‡åˆ°å…¨æ–‡åˆ†æ
            if current_intent == "search" and not file_ids:
                combined_text = " ".join(router_keywords + [request.query])
                has_file_ref = _looks_like_file_reference(combined_text)
                has_recent_hint = any(hint in combined_text for hint in ["åˆšåˆšä¸Šä¼ ", "æœ€æ–°ä¸Šä¼ ", "åˆšåˆšçš„æ–‡ä»¶", "æœ€æ–°æ–‡ä»¶", "å…¨æ–‡", "åˆšæ‰ä¸Šä¼ ", "åˆšæ‰çš„æ–‡ä»¶", "å…¨éƒ¨å†…å®¹"])
                if has_file_ref or has_recent_hint:
                    current_intent = "analyze"
                    need_full_context = True

            # [NEW] Intent-Based Limit Adjustment
            if current_intent == "export":
                logger.info("ğŸ’¾ Export Intent Detected from Router. Adjusting limits.")
                top_k = 200
                needs_search = True # Ensure search is triggered
                
            is_verbatim_mode = False
            verbatim_keywords = ["å…¨éƒ¨", "ä¸€å­—ä¸å·®", "åŸæ–‡", "full content", "verbatim", "åŸæ ·"]
            if any(k in request.query for k in verbatim_keywords):
                is_verbatim_mode = True
                current_intent = "analyze" # Force analyze if verbatim is requested
                need_full_context = True
                logger.info("âš¡ Verbatim Mode Triggered")

            # å…œåº•ï¼šRouter è¯¯åˆ¤ä¸º chat ä½†ç”¨æˆ·æåˆ°â€œåˆšæ‰/åˆšåˆš/æœ€è¿‘ ä¸Šä¼ çš„å‘ç¥¨/æ–‡ä»¶â€ï¼Œå¼ºåˆ¶èµ°æ£€ç´¢
            if current_intent == "chat":
                # [Hack] Detect "Yes/Confirm" to trigger export if context implies it?
                # This is tricky without state. We'll rely on User saying "ç¡®è®¤ä¸‹è½½" which Router picks up as Export.
                # If Router is smart, "Yes, download it" -> intent: export.
                
                recent_hints = ["åˆšæ‰ä¸Šä¼ ", "åˆšåˆšä¸Šä¼ ", "æœ€è¿‘ä¸Šä¼ ", "åˆšæ‰çš„å‘ç¥¨", "æœ€æ–°å‘ç¥¨", "åˆšæ‰çš„æ–‡ä»¶", "åˆšåˆšçš„æ–‡ä»¶", "å…¨éƒ¨å†…å®¹"]
                if any(h in request.query for h in recent_hints):
                    current_intent = "search"
                    needs_search = True
                    need_retrieval = True
                    router_keywords = list(set(router_keywords + [request.query]))
                    logger.info("ğŸ”„ Router åˆ¤ä¸º chat ä½†æ£€æµ‹åˆ°è¿‘æœŸä¸Šä¼ è¯­ä¹‰ï¼Œå¼ºåˆ¶å¯ç”¨æ£€ç´¢")

            if current_intent in ["analyze", "file_read"]:
                need_full_context = True

            top_k = 5
            time_range = router_filters.get("time_range") if isinstance(router_filters, dict) else None
            router_file_type = router_filters.get("file_type") if isinstance(router_filters, dict) else None
            need_retrieval = needs_search or bool(router_keywords)

            # å¦‚æœå°šæœªè¯†åˆ«å‡º file_idsï¼Œå°è¯•ç”¨æ–‡ä»¶å/å…³é”®è¯ç‰‡æ®µåŒ¹é…æ•°æ®åº“
            if current_intent in ["analyze", "file_read", "search"] and not file_ids:
                lookup_terms = list(router_keywords or [])
                lookup_terms.append(request.query)
                matched_ids = _find_file_ids_by_terms(db, current_user_id, lookup_terms, limit=3)
                if matched_ids:
                    file_ids = matched_ids
                    logger.info(f"ğŸ” é€šè¿‡æ–‡ä»¶ååŒ¹é…è·å¾— file_ids={file_ids}")

            if current_intent == "analyze" and not file_ids:
                latest = (
                    db.query(ArchiveRecord)
                    .filter(ArchiveRecord.user_id == current_user_id)
                    .order_by(ArchiveRecord.processed_at.desc())
                    .first()
                )
                if latest:
                    file_ids = [latest.id]
                    logger.info(f"ğŸ“„ è‡ªåŠ¨å®šä½æœ€æ–°æ–‡ä»¶: {latest.id} ({latest.filename})")

            logger.info(
                f"âœ… Router Result: intent={current_intent}, search={needs_search}, "
                f"confidence={confidence:.2f}, keywords={len(router_keywords)}, files={file_ids}"
            )
            logger.info(f"â”â”â” PHASE 2: CONTEXT BUILDING â”â”â”")
        else:
            # ä½¿ç”¨æ—§çš„ parse_intent ç»“æœï¼ˆé™çº§æ¨¡å¼ï¼‰
            intent = await router_agent.parse_intent(request.query)
            search_params = intent.get("search_params", {}) if isinstance(intent, dict) else {}
            router_keywords = search_params.get("keywords") or []
            file_ids = search_params.get("file_ids") or []
            top_k = search_params.get("top_k") or 3
            time_range = search_params.get("time_range") or None
            router_file_type = None
            current_intent = intent.get("intent", "chat") if isinstance(intent, dict) else "chat"
            
            # å¦‚æœç”¨æˆ·è¯´"åˆ†æåˆšæ‰çš„æ–‡ä»¶"ç­‰æœªæŒ‡å®š file_idsï¼Œè‡ªåŠ¨æŠ“å–å½“å‰ç”¨æˆ·æœ€æ–°ä¸Šä¼ 
            if current_intent == "analyze" and not file_ids:
                latest = (
                    db.query(ArchiveRecord)
                    .filter(ArchiveRecord.user_id == current_user_id)
                    .order_by(ArchiveRecord.processed_at.desc())
                    .first()
                )
                if latest:
                    file_ids = [latest.id]
            
            need_retrieval = current_intent in ["search", "analyze", "file_read"] or bool(router_keywords or file_ids)

        docs = []
        sources_lines = []
        
        # --- Helper: Export Handler ---
        def _execute_export_logic(f_ids):
            try:
                export_service = ExportService(db=db)
                relative_path = export_service.export_as_markdown(f_ids, title=f"Export: {request.query[:20]}...")
                public_url = get_file_public_url(relative_path)
                
                export_msg = (
                    f"âœ… **å·²ä¸ºæ‚¨å®Œæˆæ‰¹é‡å¯¼å‡º**\n\n"
                    f"å…±åˆå¹¶äº† {len(f_ids)} ä»½æ–‡æ¡£ï¼ŒåŒ…å«å®Œæ•´çš„å…ƒæ•°æ®ä¸æ­£æ–‡ã€‚\n\n"
                    f"ğŸ‘‰ **[ç‚¹å‡»ä¸‹è½½èåˆåçš„ Markdown æ–‡ä»¶]({public_url})**\n\n"
                    f"æ‚¨å¯ä»¥å°†æ­¤æ–‡ä»¶ç”¨äºå­˜æ¡£ï¼Œæˆ–å‘é€ç»™æ›´å¼ºå¤§çš„æ¨ç†æ¨¡å‹è¿›è¡Œæ·±åº¦åˆ†æã€‚"
                )
                
                # Save Interaction
                ai_msg = ChatMessage(
                    role="assistant",
                    content=export_msg,
                    model_id="system_export_service",
                    session_id=session_id,
                    user_id=current_user_id
                )
                db.add(ai_msg)
                db.commit()
                
                return {
                    "reply": export_msg,
                    "session_id": session_id,
                    "model_id": "system_export_service"
                }
            except Exception as export_err:
                logger.error(f"Export failed: {export_err}", exc_info=True)
                return None

        try:
            if current_intent == "analyze" and file_ids:
                # [Phase 5] Export Check (Direct IDs)
                if current_intent == "export" or (current_intent == "analyze" and any(k in request.query for k in ["ä¸‹è½½", "å¯¼å‡º"])):
                     export_result = _execute_export_logic(file_ids)
                     if export_result:
                        return export_result
                
                # [Phase 2] Interactive Limit Check (Human-in-the-loop)
                # Skip limit check if export? No, user might still want to export 100 files but not "analyze" them in chat.
                # If export was requested, we ALREADY returned above.
                # So if we are here, it means NOT export, or export failed.
                
                MAX_AUTO_ANALYZE = 5
                if len(file_ids) > MAX_AUTO_ANALYZE:
                    logger.info(f"ğŸ›‘ Too many files for auto-analysis: {len(file_ids)} > {MAX_AUTO_ANALYZE}")
                    
                    # Fetch metadata for top files
                    targets = (
                        db.query(ArchiveRecord)
                        .filter(ArchiveRecord.id.in_(file_ids))
                        .limit(20)  # Cap for display
                        .all()
                    )
                    
                    file_list_str = "\n".join([f"- **{t.filename}** (ID: {t.id})" for t in targets])
                    remaining_count = len(file_ids) - len(targets)
                    if remaining_count > 0:
                        file_list_str += f"\n- ... (è¿˜æœ‰ {remaining_count} ä¸ªæ–‡ä»¶)"

                    clarification_msg = (
                        f"ğŸ¤” **éœ€è¿›ä¸€æ­¥ç¡®è®¤**\n\n"
                        f"æ‚¨é€šè¿‡å…³é”®è¯åŒ¹é…åˆ°äº† {len(file_ids)} ä»½æ–‡ä»¶ï¼Œä¸€æ¬¡æ€§åˆ†æè¿™ä¹ˆå¤šæ–‡ä»¶å¯èƒ½ä¼šå¯¼è‡´ä¿¡æ¯è¿‡è½½æˆ–å›ç­”ä¸ç²¾å‡†ã€‚\n\n"
                        f"**åŒ¹é…åˆ°çš„æ–‡ä»¶ï¼ˆå‰ {len(targets)} ä¸ªï¼‰**ï¼š\n"
                        f"{file_list_str}\n\n"
                        f"**å»ºè®®**ï¼š\n"
                        f"- è¯·å‘Šè¯‰æˆ‘å…·ä½“æƒ³åˆ†æå“ªä¸€ä»½ï¼ˆä¾‹å¦‚ï¼šâ€œåˆ†æç¬¬1ä¸ªâ€ æˆ– â€œåˆ†æ ID {targets[0].id}â€ï¼‰ã€‚\n"
                        f"- æˆ–è€…ç¼©å°èŒƒå›´ï¼ˆä¾‹å¦‚ï¼šâ€œåˆ†æ2023å¹´çš„...â€ï¼‰ã€‚"
                    )
                    
                    # Save as AI message
                    ai_msg = ChatMessage(
                        role="assistant",
                        content=clarification_msg,
                        model_id="system_interactive_check",
                        session_id=session_id,
                        user_id=current_user_id
                    )
                    db.add(ai_msg)
                    db.commit()
                    
                    return {
                        "reply": clarification_msg,
                        "session_id": session_id,
                        "model_id": "system_interactive_check"
                    }

                # ç²¾è¯»åœºæ™¯ï¼šç›´æ¥æ³¨å…¥å…¨æ–‡ï¼Œè·³è¿‡æ£€ç´¢
                docs = db.query(ArchiveRecord).filter(ArchiveRecord.id.in_(file_ids)).all()
                
                # [Circuit Breaker]
                # Calculate total size to prevent context window explosion
                SAFE_TOKEN_LIMIT = 32000
                total_estimated_tokens = 0
                for doc in docs:
                    if getattr(doc, "full_text", None):
                        total_estimated_tokens += estimate_token_count(doc.full_text)
                
                if total_estimated_tokens > SAFE_TOKEN_LIMIT:
                    logger.warning(f"ğŸ›¡ï¸ Circuit Breaker Triggered: {total_estimated_tokens} > {SAFE_TOKEN_LIMIT}")
                    
                    error_message = (
                        f"ğŸš« **ä¸ºäº†é˜²æ­¢ç³»ç»Ÿè¿‡è½½ï¼Œå·²è§¦å‘å®‰å…¨ç†”æ–­**\n\n"
                        f"æ‚¨é€‰æ‹©çš„æ–‡ä»¶æ€»å†…å®¹è¿‡å¤§ï¼ˆçº¦ {total_estimated_tokens} tokensï¼‰ï¼Œè¶…è¿‡äº†å•æ¬¡ç²¾è¯»çš„å®‰å…¨é™åˆ¶ ({SAFE_TOKEN_LIMIT} tokens)ã€‚\n\n"
                        f"**å»ºè®®æ“ä½œ**ï¼š\n"
                        f"1. **å‡å°‘æ–‡ä»¶æ•°é‡**ï¼šå°è¯•ä¸€æ¬¡åªåˆ†æ 1-2 ä¸ªæ–‡ä»¶ã€‚\n"
                        f"2. **ä½¿ç”¨æ£€ç´¢æ¨¡å¼**ï¼šæ‚¨å¯ä»¥é’ˆå¯¹å…·ä½“é—®é¢˜æé—®ï¼ˆå¦‚â€œåˆåŒä¸­çš„ä»˜æ¬¾æ¡æ¬¾æ˜¯ä»€ä¹ˆâ€ï¼‰ï¼Œæˆ‘ä¼šè‡ªåŠ¨æ£€ç´¢ç›¸å…³æ®µè½ï¼Œè€Œä¸æ˜¯åŠ è½½å…¨æ–‡ã€‚"
                    )
                    
                    # Save as AI message
                    ai_msg = ChatMessage(
                        role="assistant",
                        content=error_message,
                        model_id="system_circuit_breaker",
                        session_id=session_id,
                        user_id=current_user_id
                    )
                    db.add(ai_msg)
                    db.commit()
                    
                    return {
                        "reply": error_message,
                        "session_id": session_id,
                        "model_id": "system_circuit_breaker"
                    }
                context_lines = []
                for doc in docs:
                    if getattr(doc, "full_text", None):
                        context_lines.append(
                            f"FULL CONTENT [{doc.id}] {doc.filename}:\n---\n{doc.full_text}\n---\n"
                        )
                    if doc.relative_path:
                        try:
                            public_url = get_file_public_url(doc.relative_path)
                            sources_lines.append(f"> ğŸ“ **æºæ–‡ä»¶**: [ğŸ“„ {doc.filename}]({public_url})")
                        except Exception as url_err:
                            logger.warning(f"æ„é€ æºæ–‡ä»¶é“¾æ¥å¤±è´¥ id={doc.id}: {url_err}")
                context_text = "\n".join(context_lines)
            elif need_retrieval:
                retrieval = RetrievalAgent(db=db)
                hit_ids = []
                if file_ids:
                    hit_ids = file_ids
                else:
                    hits = retrieval.hybrid_search(
                        request.query,
                        keywords=router_keywords,
                        top_k=top_k,
                        user_id=current_user_id,
                        time_range=time_range,
                        file_type=router_file_type,
                    )
                    hit_ids = [h.get("id") or h.get("doc_id") for h in hits if h.get("id") or h.get("doc_id")]
                    # è®°å½•æ£€ç´¢å‘½ä¸­è¯¦æƒ…ï¼Œä¾¿äºæ’æŸ¥
                    hit_logs = []
                    for h in hits:
                        meta = h.get("metadata") or {}
                        hit_logs.append(
                            f"id={h.get('id') or h.get('doc_id')} score={h.get('score')} file={meta.get('filename')} path={meta.get('path')}"
                        )
                    if hit_logs:
                        logger.info(f"ğŸ” æ£€ç´¢å‘½ä¸­: {' | '.join(hit_logs)}")

                # å°†å‘½ä¸­ ID è§„èŒƒä¸º intï¼Œé¿å…å­—ç¬¦ä¸²å¯¼è‡´æŸ¥è¯¢å¤±è´¥
                normalized_hit_ids = []
                for hid in hit_ids:
                    try:
                        normalized_hit_ids.append(int(hid))
                    except Exception:
                        continue
                hit_ids = normalized_hit_ids

                if hit_ids:
                    # [Phase 5] Router-First Export Logic
                    if current_intent == "export":
                        # 1. Check for Confirmation
                        CONFIRM_KEYWORDS = ["ç¡®è®¤", "æ˜¯", "yes", "confirm", "ok", "å¥½çš„", "æ²¡é—®é¢˜", "ä¸‹è½½"]
                        # Simple heuristic: if query is SHORT and contains confirm words, treat as confirmed.
                        # OR if the query itself was the request "Download X", we might ask for confirmation.
                        # Proposed flow: User asks "Download X" -> System: "Found X. Confirm?" -> User: "Yes"
                        
                        # BUT: If the user says "Download X" (initial request), we should PROMPT for confirmation.
                        # How to distinguish "Initial Request" from "User Confirmation"?
                        # We use the 'recent_messages' context. If the LAST AI message was a confirmation request, then this is a confirmation.
                        # For simplicity in this iteration: We ALWAYS prompt for confirmation UNLESS the user explicitly adds "Force" (which they won't).
                        
                        # Better approach: We check if the user is replying to a confirmation request.
                        # But here, we are stateless in this function scope.
                        # Let's assume:
                        # - If query implies "Yes/Confirm", we execute.
                        # - If query implies "Download X", we ask.
                        
                        is_confirmation = any(k in request.query.lower() for k in CONFIRM_KEYWORDS) and len(request.query) < 10
                        
                        # However, since the Router classified this current query as 'export', it means the USER INPUT was "Download X".
                        # So it is likely the Initial Request.
                        # User: "Download reports" -> Intent: Export -> Ask Confirm.
                        # User: "Yes" -> Router might classify as 'chat' (or smalltalk).
                        # So we need the Router to classify "Yes" as 'confirm_export' or we handle it in 'chat' fallback?
                        # ACTUALLY: The user wants a "Smart Interaction".
                        
                        # Improved Logic:
                        # 1. Show the preview (Ask for confirmation).
                        # 2. Add specific suggestions/buttons (if UI supported).
                        # For now, we return a text prompt.
                        
                        # WAIT: If user says "Yes" to a previous question, Router might classify it as "chat". 
                        # We need to handle that state. But for now, let's implement the "Ask" part.
                        
                        preview_files = db.query(ArchiveRecord).filter(ArchiveRecord.id.in_(hit_ids[:5])).all()
                        preview_list = "\n".join([f"- {f.filename} ({f.created_at.strftime('%Y-%m-%d')})" for f in preview_files])
                        total_count = len(hit_ids)
                        
                        confirm_msg = (
                            f"ğŸ“¦ **å‡†å¤‡æ‰“åŒ…å¯¼å‡º**\n\n"
                            f"æˆ‘æ‰¾åˆ°äº† **{total_count}** ä»½ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶ï¼ˆå…³é”®è¯: {router_keywords}ï¼‰ã€‚\n"
                            f"é¢„è§ˆå‰ 5 ä¸ªï¼š\n{preview_list}\n\n"
                            f"â“ **æ‚¨å¯ä»¥å›å¤â€œç¡®è®¤â€æˆ–â€œç«‹å³ä¸‹è½½â€æ¥å¼€å§‹åˆå¹¶ã€‚**"
                        )
                        
                        ai_msg = ChatMessage(
                            role="assistant", 
                            content=confirm_msg,
                            model_id="system_export_confirm",
                            session_id=session_id,
                            user_id=current_user_id
                        )
                        db.add(ai_msg)
                        db.commit()
                        return {
                            "reply": confirm_msg,
                            "session_id": session_id,
                            "model_id": "system_export_confirm"
                        }

                    docs = db.query(ArchiveRecord).filter(ArchiveRecord.id.in_(hit_ids)).all()
                    context_lines = []
                    for doc in docs:
                        try:
                            meta = doc.meta_data if isinstance(getattr(doc, "meta_data", None), dict) else {}
                            semantic = meta.get("semantic", {}) if isinstance(meta, dict) else {}
                            summary_from_meta = semantic.get("summary") if isinstance(semantic, dict) else None
                            cat = doc.category or semantic.get("category") or ""
                            subcat = doc.subcategory or ""
                            full_text = doc.full_text or ""
                            snippet = doc.summary or summary_from_meta or (full_text[:500] + "..." if full_text else "")

                            # ç»“æ„åŒ–ä¸Šä¸‹æ–‡ï¼Œæ ‡æ³¨ OCR/è§†è§‰å†…å®¹
                            block_lines = [
                                f"[FILE RECORD: {doc.filename}]",
                                f"[METADATA]: category={cat} subcategory={subcat} summary={snippet}",
                            ]
                            if doc.file_type == "Images" and full_text:
                                block_lines.append("[VISUAL CONTENT / OCR EXTRACT]:")
                                block_lines.append(full_text)
                            elif full_text:
                                block_lines.append("[CONTENT]:")
                                block_lines.append(full_text)
                            block_lines.append("[END OF FILE RECORD]")

                            context_lines.append("\n".join(block_lines))

                            logger.info(
                                f"ğŸ“„ ä¸Šä¸‹æ–‡æ³¨å…¥: id={doc.id} file={doc.filename} size={doc.file_size} chars={len(full_text)}"
                            )
                            if doc.relative_path:
                                try:
                                    public_url = get_file_public_url(doc.relative_path)
                                    sources_lines.append(f"> ğŸ“ **æºæ–‡ä»¶**: [ğŸ“„ {doc.filename}]({public_url})")
                                except Exception as url_err:
                                    logger.warning(f"æ„é€ æºæ–‡ä»¶é“¾æ¥å¤±è´¥ id={doc.id}: {url_err}")
                        except Exception as doc_err:
                            logger.warning(f"è·³è¿‡å¼‚å¸¸æ–‡æ¡£ {getattr(doc, 'id', 'unknown')}: {doc_err}")
                            continue
                    context_text = "\n\n".join(context_lines)
                    logger.info(f"æ£€ç´¢åˆ° {len(docs)} æ¡ä¸Šä¸‹æ–‡ï¼Œä¾›æ¨ç†æ¨¡å‹ä½¿ç”¨")
        except Exception as e:
            logger.warning(f"æ£€ç´¢é˜¶æ®µå¼‚å¸¸ï¼Œè·³è¿‡æ£€ç´¢: {e}", exc_info=True)

        # [Smart Post-Retrieval Logic]
        # Branch 1: The Guardrail (Empty Results -> Fallback & Hesitation)
        # Intercept if retrieval occurred but yielded NO documents.
        if (need_retrieval or current_intent in ["search", "analyze"]) and not docs:
            hesitation_reply = ""
            # 1. Analyze constraints
            f_type = router_filters.get("file_type")
            t_range = router_filters.get("time_range")
            
            # 2. Fallback: Query Latest Files (Sorting)
            # If search failed (e.g. strict boolean match), try to show what DOES exist.
            try:
                fallback_query = db.query(ArchiveRecord).filter(
                    ArchiveRecord.user_id == current_user_id
                ).order_by(desc(ArchiveRecord.processed_at)).limit(3)
                
                latest_files = fallback_query.all()
            except Exception as fallback_err:
                logger.error(f"Fallback query failed: {fallback_err}")
                latest_files = []

            # 3. Construct Hesitation Response
            if latest_files:
                file_list_text = "\n".join([f"- {f.filename} ({f.processed_at.strftime('%Y-%m-%d %H:%M')})" for f in latest_files])
                
                if t_range:
                    hesitation_reply = (
                        f"æˆ‘åœ¨ã€æœ€è¿‘ä¸Šä¼  ({t_range})ã€‘çš„æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚\n\n"
                        f"ä¸è¿‡ï¼Œæ‚¨æœ€è¿‘ä¸Šä¼ çš„æ–‡ä»¶å¦‚ä¸‹ï¼ˆæŒ‰æ—¶é—´æ’åºï¼‰ï¼š\n{file_list_text}\n\n"
                        f"æ˜¯å¦éœ€è¦æˆ‘åˆ†æå…¶ä¸­æŸä¸ªæ–‡ä»¶ï¼Ÿ"
                    )
                elif f_type and f_type != "All":
                    hesitation_reply = (
                        f"æˆ‘æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ã€{f_type}ã€‘ç±»å‹çš„æ–‡ä»¶ã€‚\n\n"
                        f"è¿™æ˜¯æ‚¨æœ€æ–°çš„æ–‡ä»¶ï¼š\n{file_list_text}"
                    )
                else:
                    hesitation_reply = (
                        f"æˆ‘çš„çŸ¥è¯†åº“é‡Œæ²¡æœ‰æ‰¾åˆ°ç¡®åˆ‡åŒ¹é…çš„æ–‡æ¡£ã€‚\n\n"
                        f"æ‚¨å¯èƒ½æƒ³æŸ¥çœ‹è¿™äº›æœ€è¿‘çš„æ–‡ä»¶ï¼š\n{file_list_text}"
                    )
            else:
                # Totally empty KB
                hesitation_reply = "æˆ‘çš„çŸ¥è¯†åº“é‡Œç›®å‰æ²¡æœ‰ä»»ä½•æ–‡ä»¶ã€‚è¯·å…ˆä¸Šä¼ æ–‡ä»¶ã€‚"

            logger.info(f"ğŸ›‘ Post-Retrieval Hesitation Triggered (with Fallback): {hesitation_reply[:100]}...")
            
            # 4. Save and Return (Bypass LLM)
            hesitation_msg = ChatMessage(
                role="assistant",
                content=hesitation_reply,
                model_id="rule_hesitation",
                session_id=session_id,
                user_id=current_user_id
            )
            db.add(hesitation_msg)
            db.commit()
            
            return {
                "reply": hesitation_reply,
                "session_id": session_id,
                "model_id": "rule_hesitation"
            }

        # Task 4: Context Memory - ä½¿ç”¨æ»šåŠ¨æ‘˜è¦å’Œæ»‘åŠ¨çª—å£æ„å»ºå¸¦è®°å¿†çš„ä¸Šä¸‹æ–‡ï¼ˆå•ä¸€ç³»ç»Ÿæ¶ˆæ¯åŸåˆ™ï¼‰
        memory_messages = []
        memory_summary = ""
        memory_context = ""
        try:
            # ä½¿ç”¨ä¹‹å‰åˆ›å»ºçš„ memory_serviceï¼ˆé¿å…é‡å¤åˆ›å»ºï¼‰
            # ä½¿ç”¨ Context Memory æ„å»ºä¸Šä¸‹æ–‡ï¼ˆè¿”å›æ¶ˆæ¯åˆ—è¡¨ã€æ‘˜è¦æ–‡æœ¬ã€å¯¹è¯ä¸Šä¸‹æ–‡ï¼‰
            memory_messages, memory_summary, memory_context = await memory_service.build_context_with_memory(
                session_id=session_id,
                current_query=request.query,
                base_context=context_text,  # æ£€ç´¢ç»“æœä½œä¸ºåŸºç¡€ä¸Šä¸‹æ–‡
                exclude_last_n=1  # æ’é™¤å½“å‰æ­£åœ¨å¤„ç†çš„ç”¨æˆ·æ¶ˆæ¯
            )
            logger.info("âœ… Context Memory Built")
            logger.info(f"â”â”â” PHASE 2: COMPLETE â”â”â”")
            
            # å¦‚æœ Neural Router ç”Ÿæˆäº† memory_distillationï¼Œå¯ä»¥åœ¨è¿™é‡Œä½¿ç”¨æˆ–ä¿å­˜
            if memory_distillation:
                logger.info(f"ğŸ“ Memory Distillation: {memory_distillation}")
        except Exception as e:
            logger.error(f"âŒ ä¸Šä¸‹æ–‡è®°å¿†æ„å»ºå¤±è´¥ï¼Œé™çº§ä½¿ç”¨ç®€å•ä¸Šä¸‹æ–‡: {e}", exc_info=True)
            # é™çº§ï¼šä½¿ç”¨ç©ºå€¼ï¼Œåç»­ä¼šä½¿ç”¨ç®€å•æ—¶é—´æ³¨å…¥é€»è¾‘
            memory_messages = []
            memory_summary = ""
            memory_context = context_text  # ä¿ç•™æ£€ç´¢ç»“æœ
        
        # Branch 2: The Refiner (Non-Empty Results -> Smart Instructions)
        # æ„å»ºå•ä¸€ç³»ç»Ÿæç¤ºï¼ˆéµå¾ªå•ä¸€ç³»ç»Ÿæ¶ˆæ¯åŸåˆ™ï¼‰
        system_parts = []
        
        # 1. åŸºç¡€ç³»ç»Ÿæç¤º
        system_parts.append(
            "ä½ æ˜¯æ™ºèƒ½åŠ©æ‰‹ Memexã€‚ä½ å¯ä»¥è®¿é—®å¹¶ä½¿ç”¨ä¸‹é¢æä¾›çš„ä¸Šä¸‹æ–‡ï¼ˆç”¨æˆ·æ–‡ä»¶ï¼‰ã€‚"
        )
        
        # 2. Refinement Instructions (æ€ç»´é“¾/å»é‡/å†²çªè§£å†³)
        if docs:
            refinement_instructions = """
ã€æ€ç»´é“¾è¦æ±‚ã€‘:
1. **Relevance Check**: é¦–å…ˆï¼Œåœ¨å†…å¿ƒè¯„ä¼°æ£€ç´¢åˆ°çš„ Context æ˜¯å¦çœŸçš„å›ç­”äº†ç”¨æˆ·é—®é¢˜ã€‚
2. **Synthesis**: å¦‚æœæœ‰å¤šä¸ªåˆ‡ç‰‡ï¼Œè¯·å°†å®ƒä»¬çš„ä¿¡æ¯è¿›è¡Œæ‹¼å›¾å’Œå»é‡ï¼Œä¸è¦æœºæ¢°å¤è¿°ã€‚
3. **Conflict Resolution**: å¦‚æœåˆ‡ç‰‡ä¿¡æ¯æœ‰å†²çªï¼ˆå¦‚ä¸åŒæ—¥æœŸçš„ç‰ˆæœ¬ï¼‰ï¼Œè¯·ä»¥æ—¶é—´æœ€æ–°çš„ä¸ºå‡†å¹¶è¯´æ˜ã€‚
4. **Answer**: åŸºäºä¸Šè¿°æ•´ç†ï¼Œç»™å‡ºæœ€ç»ˆå›ç­”ã€‚
"""
            system_parts.append(refinement_instructions)

        # 3. [NEW] Verbatim Mode & Multi-Doc Logic
        if is_verbatim_mode:
            system_parts.append(
                "**VERBATIM PROTOCOL**: User requested FULL/RAW content. "
                "Output the content of the file EXACTLY as it appears in the context. "
                "Do NOT summarize, do NOT distill. "
                "If multiple files are present, list them clearly with headers."
            )
        elif docs and len(set(d.id for d in docs)) > 1:
             system_parts.append(
                "**MULTI-SOURCE HANDLING**: You have context from multiple files. "
                "Please synthesize the answer. Cite which file the info comes from if useful."
             )

        # 4. åŸºç¡€ä¸Šä¸‹æ–‡è§„åˆ™
        system_parts.append(
            "åªè¦ä¸Šä¸‹æ–‡å­˜åœ¨ï¼Œå°±ç›´æ¥åŸºäºä¸Šä¸‹æ–‡å›ç­”ã€‚**ä¸è¦åœ¨å›å¤æœ«å°¾åˆ—å‡ºæºæ–‡ä»¶æˆ–ä¸‹è½½é“¾æ¥**ã€‚"
            "System Context: Current Server Time is " + now_str + "."
        )
        
        # 5. å†å²å¯¹è¯æ‘˜è¦
        if memory_summary:
            system_parts.append(f"[å†å²å¯¹è¯æ‘˜è¦]\n{memory_summary}")
        
        # åˆå¹¶ä¸ºå•ä¸€ç³»ç»Ÿæç¤ºæ–‡æœ¬
        system_prompt_text = "\n\n".join(system_parts)
        
        # å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«æœ€è¿‘å¯¹è¯å’Œæ£€ç´¢ç»“æœï¼‰ä¿ç•™åœ¨ context ä¸­ï¼Œæ·»åŠ åˆ°ç”¨æˆ·æ¶ˆæ¯
        context_text = memory_context if memory_context else ""
        try:
            if context_text:
                logger.info(f"ğŸ§¾ æœ€ç»ˆä¸Šä¸‹æ–‡å‰200å­—ç¬¦: {context_text[:200]}")
        except Exception:
            pass

        # 4. è°ƒç”¨ AI
        # Task 1: ç¦æ­¢æ¨ç†æ¨¡å‹è‡ªåŠ¨åˆ‡æ¢ - ä¸¥æ ¼ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„ model_id
        if not request.model_id:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹ï¼Œè¿”å›é”™è¯¯æ¶ˆæ¯ï¼ˆä½œä¸ºèŠå¤©æ¶ˆæ¯ï¼‰
            error_message = "ğŸš« é”™è¯¯: æœªæŒ‡å®šæ¨ç†æ¨¡å‹ï¼Œè¯·åœ¨å‰ç«¯é€‰æ‹©æ¨¡å‹åå†è¯•ã€‚"
            ai_msg = ChatMessage(
                role="assistant",
                content=error_message,
                model_id="system_error",
                session_id=session_id,
                user_id=current_user_id
            )
            db.add(ai_msg)
            db.commit()
            return {
                "reply": error_message,
                "session_id": session_id,
                "model_id": "system_error"
            }
        
        try:
            # Task 1: ä¸¥æ ¼ä½¿ç”¨ request.model_idï¼Œç¦æ­¢è‡ªåŠ¨åˆ‡æ¢
            # Task 5: ä¼ é€’ç³»ç»Ÿæç¤ºï¼ˆéµå¾ªå•ä¸€ç³»ç»Ÿæ¶ˆæ¯åŸåˆ™ï¼‰
            result = await ai_service.chat(
                query=request.query,
                model_id=request.model_id,  # ä¸¥æ ¼ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æ¨¡å‹
                context=context_text,
                intent=current_intent,
                file_ids=file_ids,
                system_prompt=system_prompt_text,  # ä¼ é€’åˆå¹¶åçš„ç³»ç»Ÿæç¤º
                db_session=db,
            )
            reply = result.get("reply", "")
            # Task 1: ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„ model_idï¼Œä¸è¦ä½¿ç”¨ result ä¸­çš„ model_idï¼ˆå¯èƒ½è¢«è‡ªåŠ¨åˆ‡æ¢ï¼‰
            used_model_id = request.model_id
        except ValueError as ve:
            # API Key æˆ–é…ç½®é”™è¯¯
            from src.core.error_translator import translate_ai_error
            error_msg = translate_ai_error(str(ve))
            logger.error(f"AI Service é…ç½®é”™è¯¯: {ve}", exc_info=True)
            # Task 2: é”™è¯¯ä¿¡æ¯æ˜¾æ€§åŒ– - é”™è¯¯ä½œä¸ºèŠå¤©æ¶ˆæ¯è¿”å›
            reply = f"ğŸš« æ¨¡å‹é…ç½®é”™è¯¯ï¼š{error_msg}ã€‚è¯·æ£€æŸ¥ API Key å’Œæ¨¡å‹é…ç½®ã€‚"
            used_model_id = request.model_id or "system_error"
        except Exception as e:
            from src.core.error_translator import translate_ai_error
            error_msg = str(e)
            translated_error = translate_ai_error(error_msg)
            logger.error(f"AI Service Error: {e}", exc_info=True)
            # Task 2: é”™è¯¯ä¿¡æ¯æ˜¾æ€§åŒ– - é”™è¯¯ä½œä¸ºèŠå¤©æ¶ˆæ¯è¿”å›
            if "All pool models failed" in error_msg or "æ‰€æœ‰Routeræ¨¡å‹å¤±è´¥" in error_msg:
                reply = f"ğŸš« æ‰€æœ‰å¯ç”¨æ¨¡å‹å‡å¤±è´¥ï¼š{translated_error}ã€‚è¯·æ£€æŸ¥æ¨¡å‹é…ç½®å’Œç½‘ç»œè¿æ¥ã€‚"
            elif "æ¨¡å‹æ± ä¸ºç©º" in error_msg or "pool" in error_msg.lower():
                reply = "ğŸš« æ¨¡å‹æ± æœªé…ç½®ï¼Œè¯·å…ˆé…ç½®æ¨ç†æ¨¡å‹ã€‚"
            elif "æŒ‡å®šæ¨¡å‹å¤±è´¥" in error_msg:
                reply = f"ğŸš« {translated_error}ã€‚è¯·æ£€æŸ¥æŒ‡å®šæ¨¡å‹çš„ API Key é…ç½®ã€‚"
            else:
                reply = f"ğŸš« ç³»ç»Ÿé”™è¯¯ï¼š{translated_error}"
            used_model_id = request.model_id or "system_error"
        
        # é™„åŠ æ£€ç´¢æ¥æºé“¾æ¥ï¼Œæå‡å¯è¿½æº¯æ€§
        if sources_lines:
            deduped_sources = []
            seen = set()
            for line in sources_lines:
                if line in seen:
                    continue
                deduped_sources.append(line)
                seen.add(line)
            reply = f"{reply}\n\n" + "\n".join(deduped_sources)

        # 5. ä¿å­˜ AI æ¶ˆæ¯
        ai_msg = ChatMessage(
            role="assistant", 
            content=reply, 
            model_id=used_model_id,
            session_id=session_id,
            user_id=current_user_id
        )
        db.add(ai_msg)
        db.commit()
        
        return {
            "reply": reply,
            "session_id": session_id, # Ensure frontend gets the (possibly new) UUID
            "model_id": used_model_id or "default"
        }
    except Exception as fatal:
        logger.error(f"Chat endpoint fatal error: {fatal}", exc_info=True)
        try:
            db.rollback()
        except Exception:
            pass
        safe_session = fallback_session_id or str(uuid.uuid4())
        return {
            "reply": "ç³»ç»Ÿç¹å¿™ï¼Œç¨åå†è¯•",
            "session_id": safe_session,
            "model_id": "fallback"
        }

@router.get("/chat/history", response_model=List[MessageResponse])
async def get_chat_history(
    session_id: str,
    limit: int = 50,
    db: Session = Depends(get_db)
):
    """è·å–æŒ‡å®šä¼šè¯çš„æ¶ˆæ¯å†å²"""
    try:
        messages = db.query(ChatMessage)\
            .filter(ChatMessage.session_id == session_id)\
            .order_by(ChatMessage.created_at)\
            .limit(limit)\
            .all()
        
        # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æ­£ç¡®åºåˆ—åŒ–
        result = []
        for msg in messages:
            result.append(MessageResponse(
                role=msg.role,
                content=msg.content or "",
                created_at=msg.created_at if msg.created_at else datetime.now(),
                model_id=msg.model_id
            ))
        return result
    except Exception as e:
        logger.error(f"è·å–èŠå¤©è®°å½•å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–èŠå¤©è®°å½•å¤±è´¥: {str(e)}")


@router.post("/chat/voice", response_model=ChatResponse)
async def chat_with_voice(
    file: UploadFile = File(...),
    current_user_id: int = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    è¯­éŸ³å¯¹è¯æ¥å£
    1. æ¥æ”¶éŸ³é¢‘æ–‡ä»¶
    2. è½¬å½• (STT) -> User Text
    3. èŠå¤©å¤„ç† (Chat) -> AI Text
    4. è¯­éŸ³åˆæˆ (TTS) -> AI Audio
    5. è¿”å› User Text, AI Text, å’Œ AI Audio (Base64)
    """
    temp_file_path = None
    try:
        # 1. ä¿å­˜ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶
        suffix = Path(file.filename).suffix or ".wav"
        temp_filename = f"voice_input_{uuid.uuid4()}{suffix}"
        temp_file_path = Path(settings.TEMP_DIR) / temp_filename
        
        # ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
        os.makedirs(settings.TEMP_DIR, exist_ok=True)
        
        with open(temp_file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
            
        logger.info(f"ğŸ¤ æ”¶åˆ°è¯­éŸ³è¾“å…¥ï¼Œå·²ä¿å­˜è‡³: {temp_file_path}")
        
        # 2. è°ƒç”¨ STT è½¬å½•
        ai_service = AIService()
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å¹¶æ²¡æœ‰æ˜¾å¼ä¼ é€’ session_idï¼Œå› ä¸º chat_with_memex å†…éƒ¨ä¼šå¤„ç†
        # ä½†æˆ‘ä»¬éœ€è¦å…ˆè½¬å½•ï¼Œå†è°ƒç”¨ chat é€»è¾‘
        
        # ä½¿ç”¨ ai_service.transcribe_audio è¿›è¡Œè½¬å½•
        # (å‡è®¾é…ç½®äº† audio æ¨¡å‹)
        try:
            # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œä»¥é¿å…é˜»å¡
            user_text = await run_in_threadpool(ai_service.transcribe_audio, temp_file_path, db_session=db)
        except Exception as stt_error:
            logger.error(f"STT è½¬å½•å¤±è´¥: {stt_error}")
            raise HTTPException(status_code=500, detail=f"è¯­éŸ³è½¬æ–‡å­—å¤±è´¥: {str(stt_error)}")
            
        logger.info(f"ğŸ—£ï¸ ç”¨æˆ·è¯­éŸ³è½¬å½•ç»“æœ: {user_text}")
        if not user_text.strip():
             raise HTTPException(status_code=400, detail="æœªæ£€æµ‹åˆ°è¯­éŸ³å†…å®¹")
        
        # 3. å¤ç”¨ Chat é€»è¾‘
        # æˆ‘ä»¬ä¸èƒ½ç›´æ¥è°ƒç”¨ HTTP endpointï¼Œè€Œæ˜¯å¤ç”¨å†…éƒ¨é€»è¾‘
        # ä¸ºäº†å¤ç”¨ï¼Œæˆ‘ä»¬æ„é€ ä¸€ä¸ª ChatRequest å¯¹è±¡
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å¤„ç† Sessionã€‚å¦‚æœå‰ç«¯æ²¡ä¼  Session IDï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªã€‚
        # ä½† UploadFile æ˜¯ Form dataï¼Œå‰ç«¯å¯èƒ½éœ€è¦æŠŠ session_id ä½œä¸º Form field ä¼ è¿‡æ¥
        # ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬æš‚æ—¶å‡è®¾å‰ç«¯åœ¨ Header æˆ–è€…æˆ‘ä»¬é€šè¿‡ query param æ‹¿ï¼Œæˆ–è€…å¹²è„†åœ¨è¿™é‡Œæ–°å»º/æŸ¥æ‰¾
        # å®é™…ä¸Šï¼ŒFastAPI Form data å¯ä»¥æ··ç”¨
        
        # ä¼˜åŒ–ï¼šè®©æˆ‘ä»¬çœ‹çœ‹å‰ç«¯æ€ä¹ˆä¼ ã€‚é€šå¸¸æ˜¯ FormData.append('file', blob); FormData.append('session_id', id)
        # ä½†æˆ‘ä»¬è¿™é‡Œçš„ç­¾ååªæ¥æ”¶äº† fileã€‚ä¸ºäº†æ”¯æŒ session_idï¼Œæˆ‘ä»¬éœ€è¦æ›´æ–°ç­¾åã€‚
        # ä¿®æ”¹å‡½æ•°ç­¾åæ·»åŠ  session_id: str = Form(None)
        
        # æš‚æ—¶æˆ‘ä»¬å…ˆä¸åšè¿™ä¸€æ­¥ï¼Œè€Œæ˜¯å‡è®¾æ€»æ˜¯æ–°å¯¹è¯æˆ–è€…ç”± chat_with_memex å†…éƒ¨å¤„ç†ï¼ˆä½†å†…éƒ¨éœ€è¦ Requestï¼‰
        # æ›´å¥½çš„åšæ³•æ˜¯é‡æ„ chat_with_memex æŠŠç”± Request å˜ä¸ºç”±å‚æ•°é©±åŠ¨çš„ service functionã€‚
        # é‰´äºæ—¶é—´ï¼Œæˆ‘ä»¬è¿™é‡Œç›´æ¥å®ä¾‹åŒ– ChatRequest å¹¶è°ƒç”¨ internal logic å¯èƒ½ä¼šæœ‰ dependency injection é—®é¢˜ã€‚
        # The safest way without refactoring is to copy the critical logic or call ai_service.chat directly.
        # ai_service.chat handles the core AI logic. We just need to handle Session & Message persistence.
        
        # Let's copy the persistence logic from chat_with_memex simplified.
        
        # 3.1 Session Management (Simplified for Voice)
        # å‡è®¾æ€»æ˜¯ä½¿ç”¨æœ€æ–°çš„ä¼šè¯æˆ–è€…æ–°å»º
        # æŸ¥æ‰¾æœ€è¿‘çš„ä¼šè¯
        session = db.query(ChatSession).filter(ChatSession.user_id == current_user_id).order_by(desc(ChatSession.updated_at)).first()
        if not session:
             session = ChatSession(title=user_text[:30], user_id=current_user_id)
             db.add(session)
             db.commit()
             db.refresh(session)
        
        session_id = str(session.id)
        
        # 3.2 Save User Message
        user_msg = ChatMessage(
            role="user",
            content=user_text,
            session_id=session_id,
            user_id=current_user_id,
            model_id="voice-input" 
        )
        db.add(user_msg)
        db.commit()
        
        # 3.3 Call AI Service
        # Build context similarly if needed (skipping elaborate RAG for now to speed up, or use simple context)
        # For full feature, we should replicate RAG. 
        # But let's start with direct chat first.
        
        # Invoke AI (Router/Reasoning)
        ai_response = await ai_service.chat(
            query=user_text, 
            context="", # TODO: Add RAG context if needed
            db_session=db
        )
        
        ai_text = ai_response["reply"]
        used_model = ai_response["model_id"]
        
        # 3.4 Save AI Message
        ai_msg = ChatMessage(
            role="assistant",
            content=ai_text,
            session_id=session_id,
            user_id=current_user_id,
            model_id=used_model
        )
        db.add(ai_msg)
        session.updated_at = datetime.now()
        db.commit()
        
        # 4. è¯­éŸ³åˆæˆ (TTS)
        audio_base64 = ""
        try:
            audio_data = await run_in_threadpool(ai_service.synthesize_audio, ai_text, db_session=db)
            # Convert to Base64
            audio_base64 = base64.b64encode(audio_data).decode('utf-8')
        except Exception as tts_error:
            logger.error(f"TTS åˆæˆå¤±è´¥: {tts_error}")
            # TTS å¤±è´¥ä¸åº”è¯¥é˜»æ–­æµç¨‹ï¼Œåªè¿”å›æ–‡æœ¬
            audio_base64 = ""

        # 5. è¿”å›ç»“æœ (Hack: Reuse ChatResponse but encapsulate extra data? 
        # No, better define a new response or just put it in a compatible field.
        # ChatResponse defines: reply, session_id, model_id.
        # We need to return audio. Let's return a dict/JSONResponse since ChatResponse is strict Pydantic.
        # Or we can return ChatResponse and put audio in a custom header? No.
        # Let's adjust strictness or return JSON.
        
        return {
            "reply": ai_text,
            "session_id": session_id,
            "model_id": used_model,
            "user_text": user_text, # è¿”å›è¯†åˆ«çš„ç”¨æˆ·æ–‡æœ¬
            "audio_data": audio_base64 # Base64 Audio
        }

    except Exception as e:
        logger.error(f"è¯­éŸ³å¯¹è¯å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è¯­éŸ³å¯¹è¯å¤„ç†å¤±è´¥: {str(e)}")
        
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_file_path and os.path.exists(temp_file_path):
            try:
                os.remove(temp_file_path)
            except Exception:
                pass
