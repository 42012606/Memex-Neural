"""
AI æœåŠ¡ç»Ÿä¸€å…¥å£
æ”¯æŒæ¨¡å‹æ±  (reasoning/router) + è‡ªåŠ¨æ•…éšœåˆ‡æ¢ï¼Œå…¼å®¹æ—§é…ç½®ã€‚
"""
import logging
from pathlib import Path
from typing import Optional, Dict, Any

from starlette.concurrency import run_in_threadpool

from src.core.config_manager import config_manager
from src.core.database import SessionLocal
from src.core.model_manager import model_manager
from src.core.config import settings
from src.services.ai.factory import AIProviderFactory
from src.core.prompt_manager import prompt_manager # [New]

logger = logging.getLogger(__name__)


class AIService:
    def __init__(self, service_type: str = "reasoning"):
        # service_type: reasoning / router
        self.service_type = service_type
        self._provider = None  # legacy å•æ¨¡å‹
        self._pool_providers = []  # [{"priority": int, "db_id": int, "name": str, "instance": provider}]
        self._allow_failover = self.service_type == "router"  # è·¯ç”±å…è®¸è‡ªåŠ¨åˆ‡æ¢ï¼Œæ¨ç†ä»…æŠ¥é”™
        self._user_profile_cache: Optional[str] = None

    # --- Provider builders ---
    def _get_agent_type(self) -> str:
        return "router" if self.service_type == "router" else "reasoning"

    def _build_provider(self, model, db) -> Any:
        api_key = model.api_key
        if not api_key:
            legacy_cfg = config_manager.get_config(self.service_type, db=db)
            if legacy_cfg.get("provider") == model.provider:
                api_key = legacy_cfg.get("api_key")
        
        # âœ… æ–°å¢: è¯»å–é¢„è®¾é…ç½®
        extra_config = model.config or {}
        
        return AIProviderFactory.create(
            provider_type=model.provider,
            api_key=api_key or "",
            model_id=model.model_id,
            base_url=model.base_url,
            extra_config=extra_config,  # âœ… ä¼ é€’é¢å¤–é…ç½®
        )

    def _load_pool(self, db_session=None):
        db = db_session or SessionLocal()
        try:
            active_models = model_manager.get_active_models(db, agent_type=self._get_agent_type())
            providers = []
            for m in active_models:
                try:
                    providers.append(
                        {
                            "priority": m.priority,
                            "db_id": m.id,
                            "name": m.name,
                            "instance": self._build_provider(m, db),
                        }
                    )
                except Exception as e:
                    logger.error(f"âŒ åŠ è½½æ¨¡å‹æ± é¡¹ {m.name} å¤±è´¥: {e}")
            providers.sort(key=lambda x: (x["priority"],))
            self._pool_providers = providers
            if providers:
                logger.info(f"âœ… {self._get_agent_type()} æ¨¡å‹æ± åŠ è½½å®Œæˆï¼Œå…± {len(providers)} ä¸ª")
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å‹æ± å¤±è´¥: {e}")
        finally:
            if not db_session:
                db.close()

    def _load_legacy_provider(self, db=None):
        config = config_manager.get_config(self.service_type, db=db)
        provider_type = config.get("provider", "gemini")
        api_key = config.get("api_key", "")
        model_id = config.get("model_id")
        if not api_key:
            raise ValueError(f"{self.service_type} æœåŠ¡çš„ API Key æœªé…ç½®")
        self._provider = AIProviderFactory.create(
            provider_type=provider_type,
            api_key=api_key,
            model_id=model_id,
            base_url=config.get("base_url"),
        )

    def _get_user_profile_text(self) -> str:
        """è¯»å–ç”¨æˆ·ç”»åƒï¼Œå¤´éƒ¨æ³¨å…¥åˆ° System Prompt"""
        if self._user_profile_cache is not None:
            return self._user_profile_cache
        profile_path = Path(settings.DATA_DIR) / "user_profile.md"
        try:
            text = profile_path.read_text(encoding="utf-8")
            self._user_profile_cache = text.strip()
        except Exception as e:
            logger.warning(f"è¯»å– user_profile.md å¤±è´¥: {e}")
            self._user_profile_cache = ""
        return self._user_profile_cache or ""

    # --- Core chat with failover ---
    async def chat(
        self,
        query: str,
        context: str = "",
        model_id: Optional[str] = None,
        intent: Optional[str] = None,
        file_ids: Optional[list] = None,
        system_prompt: Optional[str] = None,
        db_session=None,
    ) -> Dict[str, str]:
        from src.core.error_translator import translate_ai_error
        
        db = db_session or SessionLocal()
        try:
            profile = self._get_user_profile_text()
            # --- retry helper ---
            async def _call_with_retry(callable_fn, *, model_name: str) -> Any:
                import asyncio

                max_retries = 2
                last_error = None
                for attempt in range(max_retries):
                    try:
                        return await run_in_threadpool(callable_fn)
                    except Exception as e:
                        last_error = e
                        err_str = str(e).lower()
                        logger.warning(f"âš ï¸ AI Call Failed (Attempt {attempt+1}/{max_retries}) on {model_name}: {e}")
                        if ("api key" in err_str) or ("invalid" in err_str) or ("é¢‘ç‡" in err_str) or ("é…é¢" in err_str) or ("quota" in err_str) or ("rate" in err_str):
                            raise e
                        await asyncio.sleep(1 * (attempt + 2))  # æ‹‰é•¿é€€é¿ï¼š2s,3s
                logger.error(f"âŒ AI Service Exhausted all {max_retries} retries for {model_name}. Last error: {last_error}")
                raise last_error
            
            # æ„å»ºç³»ç»Ÿæç¤ºï¼ˆéµå¾ªå•ä¸€ç³»ç»Ÿæ¶ˆæ¯åŸåˆ™ï¼‰
            if system_prompt:
                # å¦‚æœæä¾›äº†è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºï¼Œåˆå¹¶ User Profile
                if profile:
                    final_system_prompt = f"{system_prompt}\n\n[User Profile]\n{profile}"
                else:
                    final_system_prompt = system_prompt
            else:
                # å¦‚æœæ²¡æœ‰æä¾›ç³»ç»Ÿæç¤ºï¼Œä½¿ç”¨é»˜è®¤å¹¶æ·»åŠ  User Profile
                default_system = prompt_manager.get("system.chat_default", default="ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ Memexã€‚è¯·å°½åŠ›å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœæä¾›äº†ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¯·ä¼˜å…ˆåŸºäºä¸Šä¸‹æ–‡å›ç­”ï¼Œå¹¶å¼•ç”¨æ¥æºã€‚")
                if profile:
                    final_system_prompt = f"{default_system}\n\n[User Profile]\n{profile}"
                else:
                    final_system_prompt = default_system
            
            composed_context = context or ""

            # è‹¥ä¸ºç²¾è¯»/åˆ†æåœºæ™¯ï¼Œå¼ºåˆ¶æ³¨å…¥å…¨æ–‡å†…å®¹
            if intent == "analyze" and file_ids:
                try:
                    from src.models.archive import ArchiveRecord

                    q = db.query(ArchiveRecord).filter(ArchiveRecord.id.in_(file_ids))
                    docs = q.all()
                    full_blocks = []
                    for doc in docs:
                        if getattr(doc, "full_text", None):
                            full_blocks.append(
                                f"Here is the FULL CONTENT of the file {doc.filename}:\n---\n{doc.full_text}\n---\n"
                            )
                    if full_blocks:
                        composed_context = (composed_context + "\n\n" + "\n".join(full_blocks)).strip()
                except Exception as e:
                    logger.warning(f"æ³¨å…¥å…¨æ–‡å¤±è´¥: {e}")

            # 1) ç”¨æˆ·æŒ‡å®šæ¨¡å‹æ¨¡å¼ï¼ˆReasoning/User æ¨¡å¼ï¼šç¦æ­¢ Failoverï¼Œç«‹å³æŠ¥é”™ï¼‰
            if model_id:
                try:
                    model = self._fetch_model(db, model_id)
                    provider = self._build_provider(model, db)
                    # --- DIAGNOSTIC LOGGING START ---
                    try:
                        prompt_len = len(str(query)) if query else 0
                        sys_len = len(str(final_system_prompt)) if final_system_prompt else 0
                        total_est = prompt_len + sys_len
                        logger.info(
                            f"ğŸ” AI PAYLOAD CHECK: Model={model.model_id} | Type={self.service_type} | "
                            f"PromptLen={prompt_len} | SystemLen={sys_len} | TotalChars={total_est}"
                        )
                        if total_est > 10000:
                            logger.warning("âš ï¸ MASSIVE PAYLOAD DETECTED (>10k chars)! This may cause timeouts.")
                    except Exception as diag_e:
                        logger.error(f"Diagnostic log failed: {diag_e}")
                    # --- DIAGNOSTIC LOGGING END ---
                    reply = await _call_with_retry(
                        lambda: provider.chat(query, composed_context, system_prompt=final_system_prompt),
                        model_name=model.name,
                    )
                    if self._is_error_reply(reply):
                        # ç”¨æˆ·æŒ‡å®šæ¨¡å‹å¤±è´¥ï¼Œç«‹å³æŠ›å‡ºé”™è¯¯ï¼ˆç¦æ­¢ Failoverï¼‰
                        error_msg = translate_ai_error(reply)
                        logger.warning(f"âš ï¸ ç”¨æˆ·æŒ‡å®šæ¨¡å‹ {model.name} å¤±è´¥: {error_msg}")
                        raise Exception(f"æŒ‡å®šæ¨¡å‹å¤±è´¥: {error_msg}")
                    return {"reply": reply, "model_id": str(model.id)}
                except Exception as e:
                    # ç”¨æˆ·æŒ‡å®šæ¨¡å‹å¤±è´¥ï¼Œç«‹å³æŠ›å‡ºé”™è¯¯ï¼ˆç¦æ­¢ Failoverï¼‰
                    error_msg = translate_ai_error(str(e))
                    logger.warning(f"âš ï¸ ç”¨æˆ·æŒ‡å®šæ¨¡å‹è°ƒç”¨å¤±è´¥: {error_msg}")
                    raise Exception(f"æŒ‡å®šæ¨¡å‹å¤±è´¥: {error_msg}")

            # 2) æ± æ¨¡å¼ + æ•…éšœåˆ‡æ¢
            if not self._pool_providers:
                self._load_pool(db_session=db)

            if self._pool_providers:
                # 2A) Router/Auto æ¨¡å¼ï¼ˆé«˜å¯ç”¨ï¼šå¿…é¡»å®ç°æ­»å¾ªç¯ Failoverï¼‰
                if self._allow_failover or self.service_type == "router":
                    errors = []
                    for item in self._pool_providers:
                        try:
                            # --- DIAGNOSTIC LOGGING START ---
                            try:
                                prompt_len = len(str(query)) if query else 0
                                sys_len = len(str(final_system_prompt)) if final_system_prompt else 0
                                total_est = prompt_len + sys_len
                                logger.info(
                                    f"ğŸ” AI PAYLOAD CHECK: Model={item['name']} | Type={self.service_type} | "
                                    f"PromptLen={prompt_len} | SystemLen={sys_len} | TotalChars={total_est}"
                                )
                                if total_est > 10000:
                                    logger.warning("âš ï¸ MASSIVE PAYLOAD DETECTED (>10k chars)! This may cause timeouts.")
                            except Exception as diag_e:
                                logger.error(f"Diagnostic log failed: {diag_e}")
                            # --- DIAGNOSTIC LOGGING END ---
                            reply = await _call_with_retry(
                                lambda: item["instance"].chat(query, composed_context, system_prompt=final_system_prompt),
                                model_name=item["name"],
                            )
                            if self._is_error_reply(reply):
                                # è¿”å›é”™è¯¯å­—ç¬¦ä¸²ï¼Œè®°å½•å¹¶ç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªï¼ˆæ­»å¾ªç¯ Failoverï¼‰
                                error_msg = translate_ai_error(reply)
                                logger.warning(f"âš ï¸ Routeræ¨¡å‹ {item['name']} è¿”å›é”™è¯¯ï¼Œåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæ¨¡å‹: {error_msg}")
                                errors.append(f"{item['name']}: {error_msg}")
                                continue
                            # æˆåŠŸï¼Œè¿”å›ç»“æœ
                            logger.info(f"âœ… Routeræ¨¡å‹ {item['name']} è°ƒç”¨æˆåŠŸ")
                            return {"reply": reply, "model_id": str(item["db_id"])}
                        except Exception as e:
                            # Provider æŠ›å‡ºå¼‚å¸¸ï¼Œè®°å½•å¹¶ç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªï¼ˆæ­»å¾ªç¯ Failoverï¼‰
                            error_msg = translate_ai_error(str(e))
                            logger.warning(f"âš ï¸ Routeræ¨¡å‹ {item['name']} è°ƒç”¨å¤±è´¥ï¼Œåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæ¨¡å‹: {error_msg}")
                            errors.append(f"{item['name']}: {error_msg}")
                            continue
                    # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥ï¼Œæ‰æŠ›å‡ºå¼‚å¸¸
                    all_errors = "; ".join(errors) if errors else "æœªçŸ¥é”™è¯¯"
                    raise Exception(f"æ‰€æœ‰Routeræ¨¡å‹å¤±è´¥: {all_errors}")
                
                # 2B) Reasoning/User æ¨¡å¼ï¼ˆå¼ºä¸€è‡´ï¼šç¦æ­¢ Failoverï¼Œä»…å°è¯•é¦–é€‰ï¼‰
                else:
                    item = self._pool_providers[0]
                    try:
                        reply = await _call_with_retry(
                            lambda: item["instance"].chat(query, composed_context, system_prompt=final_system_prompt),
                            model_name=item["name"],
                        )
                        if self._is_error_reply(reply):
                            error_msg = translate_ai_error(reply)
                            logger.error(f"âŒ æ¨ç†æ¨¡å‹ {item['name']} è¿”å›é”™è¯¯: {error_msg}")
                            raise Exception(f"æ¨ç†æ¨¡å‹å¤±è´¥: {error_msg}")
                        return {"reply": reply, "model_id": str(item["db_id"])}
                    except Exception as e:
                        error_msg = translate_ai_error(str(e))
                        logger.error(f"âŒ æ¨ç†æ¨¡å‹ {item['name']} è°ƒç”¨å¤±è´¥: {error_msg}")
                        raise Exception(f"æ¨ç†æ¨¡å‹å¤±è´¥: {error_msg}")

            # 3) Legacy fallback
            try:
                logger.info("â„¹ï¸ æ¨¡å‹æ± ä¸ºç©ºï¼Œä½¿ç”¨ Legacy é…ç½®")
                reply = await self._chat_legacy(query, composed_context, system_prompt=final_system_prompt)
                if self._is_error_reply(reply):
                    error_msg = translate_ai_error(reply)
                    raise Exception(f"Legacy é…ç½®è¿”å›é”™è¯¯: {error_msg}")
                return {"reply": reply, "model_id": "legacy"}
            except Exception as e:
                error_msg = translate_ai_error(str(e))
                logger.error(f"âŒ Legacy é…ç½®ä¹Ÿå¤±è´¥: {error_msg}")
                raise Exception(f"æ‰€æœ‰é…ç½®è·¯å¾„å¤±è´¥: {error_msg}")
        finally:
            if not db_session:
                db.close()

    def _fetch_model(self, db, model_id: str):
        try:
            db_id = int(model_id)
        except (TypeError, ValueError):
            raise ValueError("model_id å¿…é¡»æ˜¯æœ‰æ•ˆçš„æ¨¡å‹è®°å½• ID")
        model = model_manager.get_model(db, db_id)
        if not model or not model.is_active:
            raise ValueError("æŒ‡å®šçš„æ¨¡å‹ä¸å¯ç”¨")
        expected_type = self._get_agent_type()
        if model.agent_type != expected_type:
            raise ValueError(f"æ¨¡å‹ç±»å‹ä¸åŒ¹é…ï¼ŒæœŸæœ› {expected_type}")
        return model

    async def _chat_legacy(self, query: str, context: str, system_prompt: str = None) -> str:
        if not self._provider:
            self._load_legacy_provider()
        return await run_in_threadpool(self._provider.chat, query, context, system_prompt=system_prompt)

    # --- Sync generation ---
    def generate_text(self, prompt: str, model_id: Optional[str] = None) -> str:
        from src.core.error_translator import translate_ai_error
        
        db = SessionLocal()
        try:
            if model_id:
                try:
                    model = self._fetch_model(db, model_id)
                    provider = self._build_provider(model, db)
                    result = provider.generate_text(prompt)
                    if self._is_error_reply(result):
                        error_msg = translate_ai_error(result)
                        logger.warning(f"âš ï¸ æŒ‡å®šæ¨¡å‹ç”Ÿæˆå¤±è´¥: {error_msg}")
                        raise Exception(error_msg)
                    return result
                except Exception as e:
                    error_msg = translate_ai_error(str(e))
                    logger.warning(f"âš ï¸ æŒ‡å®šæ¨¡å‹è°ƒç”¨å¤±è´¥: {error_msg}")
                    raise Exception(error_msg)

            if not self._pool_providers:
                self._load_pool(db_session=db)
            if self._pool_providers:
                errors = []
                for item in self._pool_providers:
                    try:
                        result = item["instance"].generate_text(prompt)
                        if self._is_error_reply(result):
                            error_msg = translate_ai_error(result)
                            logger.warning(f"âš ï¸ æ¨¡å‹ {item['name']} ç”Ÿæˆå¤±è´¥: {error_msg}")
                            errors.append(f"{item['name']}: {error_msg}")
                            continue
                        return result
                    except Exception as e:
                        error_msg = translate_ai_error(str(e))
                        logger.warning(f"âš ï¸ æ¨¡å‹ {item['name']} è°ƒç”¨å¤±è´¥: {error_msg}")
                        errors.append(f"{item['name']}: {error_msg}")
                        continue
                all_errors = "; ".join(errors) if errors else "æœªçŸ¥é”™è¯¯"
                raise Exception(f"æ‰€æœ‰æ¨¡å‹æ± å¤±è´¥: {all_errors}")

            if not self._provider:
                self._load_legacy_provider(db=db)
            result = self._provider.generate_text(prompt)
            if self._is_error_reply(result):
                error_msg = translate_ai_error(result)
                raise Exception(f"Legacy é…ç½®å¤±è´¥: {error_msg}")
            return result
        finally:
            db.close()

    # --- File analysis with failover ---
    def analyze_file(self, file_path: Path, model_id: Optional[str] = None, context_text: Optional[str] = None, db_session=None) -> dict:
        """
        åˆ†ææ–‡ä»¶ï¼Œæ”¯æŒæ¨¡å‹æ±  Failover
        :param file_path: æ–‡ä»¶è·¯å¾„
        :param model_id: æŒ‡å®šæ¨¡å‹ IDï¼ˆå¯é€‰ï¼‰
        :param context_text: ä¸Šä¸‹æ–‡æ–‡æœ¬ï¼ˆOCR/è½¬å½•ç»“æœï¼‰
        :param db_session: æ•°æ®åº“ä¼šè¯ï¼ˆå¯é€‰ï¼‰
        :return: åˆ†æç»“æœå­—å…¸
        """
        from src.core.error_translator import translate_ai_error
        
        db = db_session or SessionLocal()
        errors = []
        
        try:
            # 1) ç”¨æˆ·æŒ‡å®šæ¨¡å‹æ¨¡å¼ï¼ˆReasoning/User æ¨¡å¼ï¼šç¦æ­¢ Failoverï¼Œç«‹å³æŠ¥é”™ï¼‰
            if model_id:
                try:
                    model = self._fetch_model(db, model_id)
                    provider = self._build_provider(model, db)
                    result = provider.analyze_file(file_path, context_text=context_text)
                    if result and not result.get("semantic", {}).get("error"):
                        return result
                    # ç”¨æˆ·æŒ‡å®šæ¨¡å‹å¤±è´¥ï¼Œç«‹å³æŠ›å‡ºé”™è¯¯ï¼ˆç¦æ­¢ Failoverï¼‰
                    error_info = result.get("semantic", {}).get("error", "Unknown error") if result else "Analysis failed"
                    error_msg = translate_ai_error(error_info)
                    logger.error(f"âŒ ç”¨æˆ·æŒ‡å®šæ¨¡å‹ {model.name} åˆ†æå¤±è´¥: {error_msg}")
                    raise Exception(f"æŒ‡å®šæ¨¡å‹åˆ†æå¤±è´¥: {error_msg}")
                except Exception as e:
                    error_msg = translate_ai_error(str(e))
                    logger.error(f"âŒ ç”¨æˆ·æŒ‡å®šæ¨¡å‹è°ƒç”¨å¤±è´¥: {error_msg}")
                    raise Exception(f"æŒ‡å®šæ¨¡å‹å¤±è´¥: {error_msg}")
            
            # 2) æ¨¡å‹æ± æ¨¡å¼ + Failoverï¼ˆä»… reasoning ç±»å‹ï¼‰
            if not self._pool_providers:
                self._load_pool(db_session=db)
            
            if self._pool_providers:
                # 2A) Router/Auto æ¨¡å¼ï¼ˆé«˜å¯ç”¨ï¼šå¿…é¡»å®ç°æ­»å¾ªç¯ Failoverï¼‰
                # analyze_file é€šå¸¸ç”¨äºæ–‡ä»¶åˆ†æï¼Œé»˜è®¤ä½¿ç”¨ reasoning ç±»å‹ï¼Œä½†æ”¯æŒ Failover
                errors = []
                for item in self._pool_providers:
                    try:
                        result = item["instance"].analyze_file(file_path, context_text=context_text)
                        if result and not result.get("semantic", {}).get("error"):
                            logger.info(f"âœ… æ¨¡å‹ {item['name']} åˆ†ææˆåŠŸ")
                            return result
                        # å¦‚æœè¿”å›äº†é”™è¯¯ï¼Œè®°å½•å¹¶å°è¯•ä¸‹ä¸€ä¸ªï¼ˆæ­»å¾ªç¯ Failoverï¼‰
                        error_info = result.get("semantic", {}).get("error", "Unknown error") if result else "Analysis failed"
                        error_msg = translate_ai_error(error_info)
                        errors.append(f"{item['name']}: {error_msg}")
                        logger.warning(f"âš ï¸ æ¨¡å‹ {item['name']} åˆ†æå¤±è´¥ï¼Œåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæ¨¡å‹: {error_msg}")
                        continue
                    except Exception as e:
                        error_msg = translate_ai_error(str(e))
                        errors.append(f"{item['name']}: {error_msg}")
                        logger.warning(f"âš ï¸ æ¨¡å‹ {item['name']} è°ƒç”¨å¤±è´¥ï¼Œåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæ¨¡å‹: {error_msg}")
                        continue
                
                # æ‰€æœ‰æ¨¡å‹æ± éƒ½å¤±è´¥
                all_errors = "; ".join(errors) if errors else "æœªçŸ¥é”™è¯¯"
                logger.error(f"âŒ æ‰€æœ‰æ¨¡å‹æ± åˆ†æå¤±è´¥: {all_errors}")
                raise Exception(f"æ‰€æœ‰æ¨¡å‹åˆ†æå¤±è´¥: {all_errors}")
            
            # 3) Legacy fallback
            if not self._provider:
                self._load_legacy_provider(db=db)
            
            try:
                result = self._provider.analyze_file(file_path, context_text=context_text)
                if result and not result.get("semantic", {}).get("error"):
                    return result
                error_info = result.get("semantic", {}).get("error", "Unknown error") if result else "Analysis failed"
                error_msg = translate_ai_error(error_info)
                errors.append(f"Legacy: {error_msg}")
                logger.warning(f"âš ï¸ Legacy æ¨¡å‹åˆ†æå¤±è´¥: {error_msg}")
            except Exception as e:
                error_msg = translate_ai_error(str(e))
                errors.append(f"Legacy: {error_msg}")
                logger.error(f"âŒ Legacy æ¨¡å‹è°ƒç”¨å¤±è´¥: {error_msg}")
            
            # æ‰€æœ‰è·¯å¾„éƒ½å¤±è´¥
            all_errors = "; ".join(errors) if errors else "æœªçŸ¥é”™è¯¯"
            raise Exception(f"æ‰€æœ‰åˆ†æè·¯å¾„å¤±è´¥: {all_errors}")
            
        finally:
            if not db_session:
                db.close()

    # --- Helper to detect textual error and trigger failover ---
    def _is_error_reply(self, reply: Optional[str]) -> bool:
        """æ£€æµ‹å›å¤æ˜¯å¦ä¸ºé”™è¯¯ä¿¡æ¯"""
        if not isinstance(reply, str):
            return False
        lowered = reply.lower()
        error_keywords = [
            "error",
            "failed",
            "å¤±è´¥",
            "çŸ­è·¯",
            "å‘ç”Ÿé”™è¯¯",
            "api key",
            "api_key",
            "invalid",
            "unauthorized",
            "forbidden",
            "400 ",
            "401 ",
            "403 ",
            "404 ",
            "500 ",
            "502 ",
            "503 ",
            "504 ",
            "quota",
            "limit",
            "timeout",
            "connection",
        ]
        return any(k in lowered for k in error_keywords)
    
    # --- Vision/Audio/Embedding Services ---
    
    def recognize_image(self, image_url: str, db_session=None) -> str:
        """
        å›¾ç‰‡OCRè¯†åˆ«
        :param image_url: å›¾ç‰‡URLæˆ–æœ¬åœ°æ–‡ä»¶è·¯å¾„
        :param db_session: æ•°æ®åº“ä¼šè¯ï¼ˆå¯é€‰ï¼‰
        :return: æå–çš„æ–‡æœ¬å†…å®¹
        """
        db = db_session or SessionLocal()
        try:
            # è·å–Visionæ¨¡å‹
            vision_models = model_manager.get_active_models(db, agent_type="vision")
            if not vision_models:
                raise ValueError("æœªé…ç½®è§†è§‰æ¨¡å‹ï¼Œè¯·åœ¨é…ç½®é¡µé¢æ·»åŠ  Vision æ¨¡å‹")
            
            logger.info(f"Using Vision Models: {[m.name for m in vision_models]}")
            
            last_error = None
            for idx, model in enumerate(vision_models):
                try:
                    provider = self._build_provider(model, db)
                    # è°ƒç”¨DashScope Vision API
                    if hasattr(provider, 'recognize_image'):
                        result = provider.recognize_image(image_url)
                        logger.info(f"âœ… å›¾ç‰‡é€šè¿‡æ¨¡å‹ {model.name} è¯†åˆ«æˆåŠŸ")
                        return result
                    else:
                        raise ValueError(f"Provider {model.provider} ä¸æ”¯æŒå›¾ç‰‡è¯†åˆ«")
                except Exception as e:
                    error_msg = str(e)
                    logger.warning(f"âš ï¸ å›¾ç‰‡è¯†åˆ«æ¨¡å‹ {model.name} å¤±è´¥: {error_msg}")
                    last_error = error_msg
                    if idx < len(vision_models) - 1:
                        logger.info("Switching to next model...")
                    continue
            
            raise Exception(f"æ‰€æœ‰è§†è§‰æ¨¡å‹å‡å¤±è´¥ã€‚Last Error: {last_error}")

        finally:
            if not db_session:
                db.close()
    
    def transcribe_audio(self, file_path: Path, db_session=None) -> str:
        """
        éŸ³é¢‘è½¬å½•
        :param file_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        :param db_session: æ•°æ®åº“ä¼šè¯ï¼ˆå¯é€‰ï¼‰
        :return: è½¬å½•çš„æ–‡æœ¬å†…å®¹
        """
        db = db_session or SessionLocal()
        try:
            # è·å–Audioæ¨¡å‹
            audio_models = model_manager.get_active_models(db, agent_type="audio")
            if not audio_models:
                raise ValueError("æœªé…ç½®å¬è§‰æ¨¡å‹ï¼Œè¯·åœ¨é…ç½®é¡µé¢æ·»åŠ  Audio æ¨¡å‹")
            
            logger.info(f"Using Hearing Models (STT): {[m.name for m in audio_models]}")
            
            last_error = None
            for idx, model in enumerate(audio_models):
                try:
                    provider = self._build_provider(model, db)
                    if hasattr(provider, 'transcribe_audio'):
                        logger.info(f"ğŸ‘‚ Attempting transcription with {model.name}...")
                        result = provider.transcribe_audio(file_path)
                        logger.info(f"âœ… éŸ³é¢‘é€šè¿‡æ¨¡å‹ {model.name} è½¬å½•æˆåŠŸ")
                        return result
                    else:
                        raise ValueError(f"Provider {model.provider} ä¸æ”¯æŒéŸ³é¢‘è½¬å½•")
                except Exception as e:
                    error_msg = str(e)
                    logger.warning(f"âš ï¸ å¬è§‰æ¨¡å‹ {model.name} è½¬å½•å¤±è´¥: {error_msg}")
                    last_error = error_msg
                    if idx < len(audio_models) - 1:
                        logger.info("Switching to next model...")
                    continue
            
            raise Exception(f"æ‰€æœ‰å¬è§‰æ¨¡å‹(STT)å‡å¤±è´¥ã€‚Last Error: {last_error}")

        finally:
            if not db_session:
                db.close()
    
    def synthesize_audio(self, text: str, db_session=None) -> bytes:
        """
        è¯­éŸ³åˆæˆ (TTS)
        :param text: è¦åˆæˆçš„æ–‡æœ¬
        :param db_session: æ•°æ®åº“ä¼šè¯ï¼ˆå¯é€‰ï¼‰
        :return: éŸ³é¢‘äºŒè¿›åˆ¶æ•°æ®
        """
        db = db_session or SessionLocal()
        try:
            # è·å–Voiceæ¨¡å‹ (TTS)
            voice_models = model_manager.get_active_models(db, agent_type="voice")
            if not voice_models:
                raise ValueError("æœªé…ç½®è¯­éŸ³æ¨¡å‹ï¼Œè¯·åœ¨é…ç½®é¡µé¢æ·»åŠ  Voice æ¨¡å‹")
            
            logger.info(f"Using Voice Models (TTS): {[m.name for m in voice_models]}")
            
            last_error = None
            for idx, model in enumerate(voice_models):
                try:
                    provider = self._build_provider(model, db)
                    if hasattr(provider, 'synthesize_audio'):
                        logger.info(f"ğŸ”Š Attempting TTS with {model.name}...")
                        result = provider.synthesize_audio(text)
                        logger.info(f"âœ… è¯­éŸ³é€šè¿‡æ¨¡å‹ {model.name} åˆæˆæˆåŠŸ")
                        return result
                    else:
                        raise ValueError(f"Provider {model.provider} ä¸æ”¯æŒè¯­éŸ³åˆæˆ")
                except Exception as e:
                    error_msg = str(e)
                    logger.warning(f"âš ï¸ è¯­éŸ³æ¨¡å‹ {model.name} åˆæˆå¤±è´¥: {error_msg}")
                    last_error = error_msg
                    if idx < len(voice_models) - 1:
                        logger.info("Switching to next model...")
                    continue
            
            raise Exception(f"æ‰€æœ‰è¯­éŸ³æ¨¡å‹(TTS)å‡å¤±è´¥ã€‚Last Error: {last_error}")

        finally:
            if not db_session:
                db.close()
    
    def embed_text(self, text: str, db_session=None) -> list:
        """
        æ–‡æœ¬å‘é‡åŒ–
        :param text: è¾“å…¥æ–‡æœ¬
        :param db_session: æ•°æ®åº“ä¼šè¯ï¼ˆå¯é€‰ï¼‰
        :return: å‘é‡åˆ—è¡¨
        """
        db = db_session or SessionLocal()
        try:
            # è·å–Embeddingæ¨¡å‹
            embedding_models = model_manager.get_active_models(db, agent_type="embedding")
            if not embedding_models:
                raise ValueError("æœªé…ç½®è®°å¿†æ¨¡å‹ï¼Œè¯·åœ¨é…ç½®é¡µé¢æ·»åŠ  Embedding æ¨¡å‹")
            
            last_error = None
            for idx, model in enumerate(embedding_models):
                try:
                    provider = self._build_provider(model, db)
                    if hasattr(provider, 'embed_text'):
                        return provider.embed_text(text)
                    else:
                        raise ValueError(f"Provider {model.provider} ä¸æ”¯æŒæ–‡æœ¬å‘é‡åŒ–")
                except Exception as e:
                    logger.warning(f"âš ï¸ è®°å¿†æ¨¡å‹ {model.name} å‘é‡åŒ–å¤±è´¥: {e}")
                    last_error = str(e)
                    continue

            raise Exception(f"æ‰€æœ‰è®°å¿†æ¨¡å‹å‡å¤±è´¥ã€‚Last Error: {last_error}")
                
        finally:
            if not db_session:
                db.close()