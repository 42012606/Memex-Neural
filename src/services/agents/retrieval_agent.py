"""
Retrieval Agent - æ£€ç´¢ä»£ç†
è´Ÿè´£å‘é‡æ£€ç´¢ã€BM25 æ£€ç´¢ã€å¤šæ¨¡æ€ embedding
"""
import logging
import numpy as np
from typing import List, Dict, Any, Optional
from pathlib import Path
from datetime import datetime, timedelta
from sqlalchemy.orm import Session
from sqlalchemy.orm import selectinload
from src.models.archive import ArchiveRecord
from src.services.file_service import get_file_public_url

logger = logging.getLogger(__name__)


class RetrievalAgent:
    """
    æ£€ç´¢ä»£ç†
    æ”¯æŒå‘é‡æ£€ç´¢å’Œä¼ ç»Ÿå…³é”®è¯æ£€ç´¢
    """
    
    def __init__(self, db: Optional[Session] = None):
        """
        åˆå§‹åŒ–æ£€ç´¢ä»£ç†
        :param db: æ•°æ®åº“ä¼šè¯ï¼ˆå¯é€‰ï¼‰
        
        æ³¨æ„ï¼šä¸å†éœ€è¦ VectorServiceFactoryï¼Œç›´æ¥ä½¿ç”¨ AIService.embed_text() è¿›è¡Œå‘é‡åŒ–
        """
        self.db = db
        # ä¸å†éœ€è¦ _vector_serviceï¼Œç›´æ¥ä½¿ç”¨ AIService
        logger.info("âœ… Retrieval Agent åˆå§‹åŒ–å®Œæˆï¼ˆä½¿ç”¨ AIService è¿›è¡Œå‘é‡åŒ–ï¼‰")
    
    def search_by_keywords(
        self,
        keywords: List[str],
        limit: int = 5,
        user_id: int = 1,
        file_type: Optional[str] = None,
    ) -> List[ArchiveRecord]:
        """
        ä¼ ç»Ÿå…³é”®è¯æœç´¢ï¼ˆBM25 é£æ ¼ï¼Œä½¿ç”¨ SQL LIKEï¼‰
        :param keywords: å…³é”®è¯åˆ—è¡¨
        :param limit: è¿”å›ç»“æœæ•°é‡
        :param user_id: ç”¨æˆ·IDï¼Œé¢„ç•™å¤šç”¨æˆ·æ‰©å±•
        :return: å½’æ¡£è®°å½•åˆ—è¡¨
        """
        if not self.db:
            logger.warning("æ•°æ®åº“ä¼šè¯æœªæä¾›ï¼Œæ— æ³•æ‰§è¡Œå…³é”®è¯æœç´¢")
            return []
        
        try:
            query = self.db.query(ArchiveRecord).filter(
                ArchiveRecord.user_id == user_id  # [é¢„ç•™æ‰©å±•] ç”¨æˆ·éš”ç¦»
            )
            if file_type:
                query = query.filter(ArchiveRecord.file_type == file_type)
            
            # æ„å»º OR æ¡ä»¶ï¼ˆä»»æ„å…³é”®è¯åŒ¹é…ï¼‰
            from sqlalchemy import or_
            conditions = []
            for keyword in keywords:
                conditions.append(ArchiveRecord.filename.like(f"%{keyword}%"))
                conditions.append(ArchiveRecord.summary.like(f"%{keyword}%"))
                conditions.append(ArchiveRecord.category.like(f"%{keyword}%"))
                conditions.append(ArchiveRecord.full_text.like(f"%{keyword}%")) # [Fix] Search in OCR/FullText content
            
            if conditions:
                query = query.filter(or_(*conditions))
            
            results = query.order_by(ArchiveRecord.id.desc()).limit(limit).all()
            logger.info(f"âœ… å…³é”®è¯æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} æ¡ç»“æœ")
            return results
        except Exception as e:
            logger.error(f"âŒ å…³é”®è¯æœç´¢å¤±è´¥: {e}")
            return []
    
    
    def search_by_vector(
        self,
        query_text: str,
        top_k: int = 5,
        filters: Optional[Dict[str, Any]] = None,
        user_id: int = 1,
        min_score: float = 0.45  # [Threshold] Filter out irrelevant results (orthogonal ~0.41)
    ) -> List[Dict[str, Any]]:
        """
        å‘é‡ç›¸ä¼¼åº¦æœç´¢ V2 (æ”¯æŒ Parent-Child Indexing)
        
        ç­–ç•¥:
        1. ä¼˜å…ˆæœç´¢ VectorNode (Child Nodes)
        2. åŒæ—¶æœç´¢ ArchiveRecord (Parent Nodes, å…¼å®¹æ—§æ•°æ®)
        3. èšåˆç»“æœï¼Œä¼˜å…ˆå±•ç¤ºæœ€ä½³ Child Node çš„åŒ¹é…åˆ†æ•°
        """
        if not self.db:
            return []
        
        try:
            # 1. å‘é‡åŒ–æŸ¥è¯¢
            from src.services.ai_service import AIService
            ai_service = AIService()
            query_vector = ai_service.embed_text(query_text, db_session=self.db)
            
            if not query_vector:
                return []

            # -------------------------------------------------------------------------
            # Query Logic A: Search Child Nodes (finest granularity)
            # -------------------------------------------------------------------------
            from src.models.vector_node import VectorNode
            from pgvector.sqlalchemy import Vector
            import numpy as np

            # Query Child Nodes
            child_query = self.db.query(VectorNode).order_by(
                VectorNode.embedding.l2_distance(query_vector)
            ).limit(top_k * 3) # Fetch more candidate chunks for aggregation
            
            child_results = child_query.all()
            
            # -------------------------------------------------------------------------
            # Query Logic B: Search Parent Archives (legacy/coarse)
            # -------------------------------------------------------------------------
            parent_query = (
                self.db.query(ArchiveRecord)
                .options(selectinload(ArchiveRecord.storage_root))
                .filter(
                    ArchiveRecord.user_id == user_id,
                    ArchiveRecord.embedding.isnot(None),
                    ArchiveRecord.is_vectorized == 1
                )
            )
            # Apply filters
            if filters:
                if "category" in filters:
                    parent_query = parent_query.filter(ArchiveRecord.category == filters["category"])
                if "file_type" in filters:
                    parent_query = parent_query.filter(ArchiveRecord.file_type == filters["file_type"])
            
            parent_results = parent_query.order_by(
                ArchiveRecord.embedding.l2_distance(query_vector)
            ).limit(top_k).all()
            
            logger.info(f"ğŸ” å‘é‡æ£€ç´¢: ChildNodes={len(child_results)}, ParentNodes={len(parent_results)}")

            # -------------------------------------------------------------------------
            # Aggregation & Merging
            # -------------------------------------------------------------------------
            aggregated_scores = {} # { archive_id: { "score": float, "snippet": str, "source": str } }
            
            # Helper to calc similarity
            def calc_score(emb):
                if emb is None: return 0.0
                dist = np.linalg.norm(np.array(emb) - np.array(query_vector))
                return 1.0 / (1.0 + dist)

            # Process Child Nodes first
            parent_ids_from_children = set()
            for child in child_results:
                pid = child.parent_archive_id
                score = calc_score(child.embedding)
                
                # Keep the BEST chunk for each parent
                if pid not in aggregated_scores or score > aggregated_scores[pid]["score"]:
                    aggregated_scores[pid] = {
                        "score": score,
                        "snippet": child.content, # Use child content as snippet
                        "source": "child_node",
                        "child_id": child.id
                    }
                    parent_ids_from_children.add(pid)

            # Process Parent Archives (Legacy)
            # Only add if score is better or not present (usually child nodes are better)
            for parent in parent_results:
                pid = parent.id
                score = calc_score(parent.embedding)
                
                if pid not in aggregated_scores:
                    # New find from coarse index
                    aggregated_scores[pid] = {
                        "score": score,
                        "snippet": parent.summary or parent.full_text[:200] if parent.full_text else "",
                        "source": "parent_vector",
                        "child_id": None
                    }
                else:
                    # If parent vector score is surprisingly better than child vector (rare), update it?
                    # Generally we trust Child Node specific match more. 
                    # But let's keep max score.
                    if score > aggregated_scores[pid]["score"]:
                         aggregated_scores[pid]["score"] = score
                         aggregated_scores[pid]["source"] = "parent_vector_boost"

            # -------------------------------------------------------------------------
            # Fetch Archive Details for Final Response
            # -------------------------------------------------------------------------
            all_ids = list(aggregated_scores.keys())
            if not all_ids:
                return []
            
            # Batch fetch needed archives
            # Ensure we fetch all records involved (including those from children only)
            records = self.db.query(ArchiveRecord).options(
                selectinload(ArchiveRecord.storage_root)
            ).filter(ArchiveRecord.id.in_(all_ids)).all()
            
            record_map = {r.id: r for r in records}
            
            final_results = []
            for pid, info in aggregated_scores.items():
                record = record_map.get(pid)
                if not record:
                    continue
                
                # [Threshold Check]
                if info["score"] < min_score:
                    continue
                
                # Apply filters post-aggr (for child nodes results)
                # (Parent query already filtered, but child query didn't check parent filters yet)
                if filters:
                     if "category" in filters and record.category != filters["category"]:
                         continue
                     if "file_type" in filters and record.file_type != filters["file_type"]:
                         continue
                
                # Construct result
                public_url = None
                try:
                    if record.relative_path:
                        public_url = get_file_public_url(record.relative_path)
                except Exception:
                    pass

                final_results.append({
                    "id": str(record.id),
                    "doc_id": str(record.id),
                    "score": float(info["score"]),
                    "metadata": {
                        "filename": record.filename,
                        "category": record.category,
                        "file_type": record.file_type,
                        "path": record.path,
                        "relative_path": record.relative_path,
                        "public_url": public_url,
                        "user_id": record.user_id,
                        # [NEW] Enhanced metadata
                        "matched_content": info["snippet"], 
                        "match_source": info["source"]
                    }
                })
            
            # Sort by score descending
            final_results.sort(key=lambda x: x["score"], reverse=True)
            
            return final_results[:top_k]

        except Exception as e:
            logger.error(f"âŒ å‘é‡æœç´¢å¤±è´¥: {e}", exc_info=True)
            return []
    
    def _parse_date(self, value: str) -> Optional[datetime]:
        """å®½æ¾è§£ææ—¥æœŸå­—ç¬¦ä¸²ï¼ˆYYYY-MM-DD / YYYY-MMï¼‰ã€‚"""
        if not value:
            return None
        try:
            return datetime.fromisoformat(value)
        except Exception:
            pass
        try:
            return datetime.fromisoformat(f"{value}-01")
        except Exception:
            return None

    def _match_time_range(self, candidate: Optional[str], time_range: Optional[str], fallback_dt: Optional[datetime]) -> bool:
        """
        ä½¿ç”¨è¯­ä¹‰æ—¥æœŸä¼˜å…ˆçš„æ—¶é—´è¿‡æ»¤ï¼š
        - candidate: meta_data.semantic_date æˆ– structured.date
        - fallback_dt: processed_at ç­‰ç³»ç»Ÿæ—¶é—´
        """
        if not time_range:
            return True

        now = datetime.now()
        start = end = None

        tr = time_range.strip()
        import re
        tr = time_range.strip()
        
        # Support lastXd and lastXh generically
        match = re.match(r"^last(\d+)([dh])$", tr)
        if match:
            num = int(match.group(1))
            unit = match.group(2)
            if unit == "d":
                # [Fix] Use calendar days (start of day), not rolling 24h
                # last1d = Today + Yesterday (since 00:00 of day before)
                start = (now - timedelta(days=num)).replace(hour=0, minute=0, second=0, microsecond=0)
            elif unit == "h":
                start = now - timedelta(hours=num)
            end = now
        elif "~" in tr:
            parts = tr.split("~", 1)
            start = self._parse_date(parts[0])
            end = self._parse_date(parts[1]) if len(parts) > 1 else None
        else:
            # å•ç‚¹æ—¥æœŸ/æœˆä»½/å¹´ä»½
            start = self._parse_date(tr)
            end = None

        def in_range(dt: datetime) -> bool:
            if start and end:
                return start <= dt <= end
            if start and not end:
                return dt >= start
            return True

        # ä¼˜å…ˆè¯­ä¹‰æ—¶é—´
        semantic_dt = self._parse_date(candidate) if candidate else None
        if semantic_dt:
            return in_range(semantic_dt)
        if fallback_dt:
            return in_range(fallback_dt)
        return True

    def hybrid_search(
        self,
        query_text: str,
        keywords: Optional[List[str]] = None,
        top_k: int = 5,
        user_id: int = 1,
        time_range: Optional[str] = None,
        file_type: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """
        æ··åˆæœç´¢ï¼ˆå‘é‡ + å…³é”®è¯ï¼‰
        :param query_text: æŸ¥è¯¢æ–‡æœ¬
        :param keywords: å…³é”®è¯åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä» query_text æå–ï¼‰
        :param top_k: è¿”å›ç»“æœæ•°é‡
        :param user_id: ç”¨æˆ·IDï¼Œé¢„ç•™å¤šç”¨æˆ·æ‰©å±•
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        # å¦‚æœå…³é”®è¯æœªæä¾›ï¼Œä»æŸ¥è¯¢æ–‡æœ¬ä¸­æå–ï¼ˆæ”¯æŒä¸­æ–‡ï¼‰
        if not keywords:
            import re
            # æå–ä¸­æ–‡å…³é”®è¯ï¼ˆ2-4å­—ï¼‰
            chinese_words = re.findall(r'[\u4e00-\u9fff]{2,4}', query_text)
            # æå–è‹±æ–‡å•è¯ï¼ˆè‡³å°‘3ä¸ªå­—ç¬¦ï¼‰
            english_words = re.findall(r'\b[a-zA-Z]{3,}\b', query_text)
            # åˆå¹¶ï¼šä¼˜å…ˆä¸­æ–‡å…³é”®è¯ï¼Œç„¶åè‹±æ–‡å•è¯
            keywords = chinese_words[:3] + english_words[:2]
            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œä½¿ç”¨ç©ºæ ¼åˆ†å‰²ï¼ˆé€‚ç”¨äºè‹±æ–‡ï¼‰æˆ–å–å‰10ä¸ªå­—ç¬¦
            if not keywords:
                keywords = query_text.split()[:5] if ' ' in query_text else [query_text[:10]]
        
        # 1. å‘é‡æœç´¢
        vector_filters = {}
        if file_type:
            vector_filters["file_type"] = file_type
        
        # å¢å¼ºæŸ¥è¯¢æ–‡æœ¬ï¼šå¦‚æœæœ‰å…³é”®è¯ï¼Œåˆå¹¶åˆ°æŸ¥è¯¢æ–‡æœ¬ä¸­ä»¥æå‡æœç´¢ç²¾åº¦
        enhanced_query = query_text
        if keywords:
            enhanced_query = f"{query_text} {' '.join(keywords)}"
            logger.debug(f"ğŸ” å¢å¼ºæŸ¥è¯¢æ–‡æœ¬: {enhanced_query} (åŸå§‹: {query_text}, å…³é”®è¯: {keywords})")
        
        # [Rerank Upgrade] Slightly reduce recall size to improve speed (from 4x to 3x)
        recall_k = top_k * 3 if top_k < 10 else top_k * 2
        vector_results = self.search_by_vector(enhanced_query, top_k=recall_k, user_id=user_id, filters=vector_filters or None)
        
        # 2. å…³é”®è¯æœç´¢
        keyword_results = self.search_by_keywords(keywords, limit=recall_k, user_id=user_id, file_type=file_type)

        # 2.5 æ—¶é—´è¿‡æ»¤ï¼ˆè¯­ä¹‰æ—¥æœŸä¼˜å…ˆï¼‰
        id_set = set()
        for v in vector_results:
            doc_id = v.get("id") or v.get("doc_id")
            if doc_id:
                id_set.add(int(doc_id))
        for k in keyword_results:
            id_set.add(int(k.id))

        records_map = {}
        if self.db and id_set:
            try:
                records = self.db.query(ArchiveRecord).filter(ArchiveRecord.id.in_(list(id_set))).all()
                records_map = {int(r.id): r for r in records}
            except Exception as e:
                logger.warning(f"æ—¶é—´è¿‡æ»¤åŠ è½½è®°å½•å¤±è´¥: {e}")

        def pass_time(doc_id: int) -> bool:
            rec = records_map.get(int(doc_id))
            if not rec:
                return True
            meta = rec.meta_data if isinstance(getattr(rec, "meta_data", None), dict) else {}
            sem_date = meta.get("semantic_date") or meta.get("structured", {}).get("date")
            return self._match_time_range(sem_date, time_range, getattr(rec, "processed_at", None))

        vector_results = [r for r in vector_results if pass_time(r.get("id") or r.get("doc_id"))]
        keyword_results = [r for r in keyword_results if pass_time(r.id)]

        # 3. åˆå¹¶ç»“æœï¼ˆå»é‡ï¼Œä¼˜å…ˆå‘é‡æœç´¢ç»“æœï¼‰
        combined = {}
        
        # è·å–æœ€å¤§ID (ç”¨äºåˆ¤æ–­æ–°æ—§ç¨‹åº¦)
        max_id = 0
        if keyword_results:
            max_id = max([r.id for r in keyword_results])
        if vector_results:
            v_max = max([int(r.get("id") or r.get("doc_id") or 0) for r in vector_results])
            max_id = max(max_id, v_max)

        # å…ˆæ·»åŠ å‘é‡æœç´¢ç»“æœ
        for result in vector_results:
            doc_id = result.get("id") or result.get("doc_id")
            if doc_id:
                combined[doc_id] = {
                    "id": doc_id,
                    "score": result.get("score", 0.0),
                    "source": "vector",
                    "metadata": result.get("metadata", {})
                }
        
        # å†æ·»åŠ å…³é”®è¯æœç´¢ç»“æœ
        for record in keyword_results:
            # [Score Tuning] Lower base score for keyword matches to trust Vector more
            base_score = 0.3  # Was 0.5
            if max_id > 0 and record.id >= max_id - 5:
                 base_score += 0.1 # Was 0.35 - Reduce recency bias to match Vector (~0.45)
            
            # [Fix] Extract snippet from full_text if possible for Reranker context
            snippet = record.summary or ""
            matched_source = "keyword_summary"
            
            if record.full_text and keywords:
                text_lower = record.full_text.lower()
                for kw in keywords:
                    idx = text_lower.find(kw.lower())
                    if idx != -1:
                        # Extract 50 chars before and 150 chars after
                        start = max(0, idx - 50)
                        end = min(len(record.full_text), idx + 150)
                        snippet = record.full_text[start:end]
                        matched_source = "keyword_fulltext_snippet"
                        # Clean up newlines for cleaner context
                        snippet = snippet.replace("\n", " ").strip()
                        break

            if record.id not in combined:
                public_url = None
                try:
                    if record.relative_path:
                        public_url = get_file_public_url(record.relative_path)
                except Exception:
                    pass
                combined[record.id] = {
                    "id": record.id,
                    "score": base_score,  
                    "source": "keyword",
                    "metadata": {
                        "filename": record.filename,
                        "category": record.category,
                        "summary": record.summary,
                        "relative_path": record.relative_path,
                        "public_url": public_url,
                        # [NEW] Inject the snippet for Reranker
                        "matched_content": snippet,
                        "match_source": matched_source
                    }
                }
            else:
                existing_score = combined[record.id]["score"]
                if base_score > existing_score:
                     combined[record.id]["score"] = base_score
                     combined[record.id]["source"] = "keyword_boost"
        
        # -------------------------------------------------------------------------
        # 4. Local Rerank (ç²¾æ’)
        # -------------------------------------------------------------------------
        candidate_items = list(combined.values())
        
        if not candidate_items:
            return []

        # å‡†å¤‡ Rerank å€™é€‰æ–‡æœ¬
        candidate_texts = []
        for item in candidate_items:
            meta = item.get("metadata", {})
            # ä¼˜å…ˆä½¿ç”¨ Child Node çš„åŒ¹é…ç‰‡æ®µï¼Œå…¶æ¬¡æ‘˜è¦ï¼Œæœ€åæ–‡ä»¶å
            text = meta.get("matched_content") or meta.get("summary") or meta.get("filename") or ""
            candidate_texts.append(text)
        
        try:
            from src.services.ai.rerank_provider import RerankService
            reranker = RerankService()
            
            # æ‰§è¡Œé‡æ’åº (Rerank)
            # æ³¨æ„: å¦‚æœ sentence-transformers æœªå®‰è£…ï¼Œreranker ä¼šè‡ªåŠ¨å›é€€åˆ°åŸå§‹é¡ºåº
            reranked_indices = reranker.rerank(query_text, candidate_texts, top_k=top_k)
            
            # Helper: Sigmoid to normalize logits to 0-1
            def sigmoid(x):
                return 1 / (1 + np.exp(-x))

            final_results = []
            for idx, score in reranked_indices:
                if idx < len(candidate_items):
                    item = candidate_items[idx]
                    
                    # Log raw score for debug
                    norm_score = sigmoid(score)
                    logger.info(f"Rerank Item {item['id']}: Raw={score:.4f}, Norm={norm_score:.4f}, Text={candidate_texts[idx][:50]}...")

                    # Update score to normalized score
                    # ä¿ç•™åŸå§‹åˆ†æ•°ä¸º original_score
                    if "original_score" not in item:
                        item["original_score"] = item["score"]
                    
                    item["score"] = float(norm_score)
                    item["metadata"]["rerank_score"] = float(norm_score)
                    
                    # [Root Cause Fix] Keyword Verification
                    # If we have explicit keywords, and this doc contains NONE of them, penalize it.
                    # This prevents "Pork Receipt" (no "Electricity") from sneaking in via "Recent" or loose vector match.
                    if keywords:
                        # Check snippets/summary/filename for keyword presence
                        content_to_check = (item.get("metadata", {}).get("matched_content") or "") + \
                                         (item.get("metadata", {}).get("summary") or "") + \
                                         (item.get("metadata", {}).get("filename") or "")
                        
                        # Loose check (case insensitive)
                        has_kw = any(k.lower() in content_to_check.lower() for k in keywords)
                        if not has_kw:
                            # Penalize vague matches.
                            # Exception: If score is SUPER high (>0.85), it might be a synonym we missed (e.g. "Power" vs "Electricity"), so trust it.
                            if float(norm_score) < 0.85:
                                logger.info(f"ğŸ“‰ Penalizing Item {item['id']} (No keyword match): {norm_score:.4f} -> {norm_score * 0.5:.4f}")
                                norm_score = norm_score * 0.5
                                item["score"] = float(norm_score)
                                item["metadata"]["rerank_score"] = float(norm_score)

                    # [Threshold Filtering] Exclude low relevance
                    # With Sigmoid: 0.5 is neutral (logit 0). 0.25 is quite lenient (logit -1.1).
                    if float(norm_score) < 0.40:  # Restored to reasonable 0.40
                         logger.info(f"Skipping Item {item['id']} (Score {norm_score:.4f} < 0.40)")
                         continue 
                    
                    final_results.append(item)
            
            logger.info(f"âœ… Rerank å®Œæˆï¼Œè¿”å› {len(final_results)} æ¡ç»“æœ (TopScore: {final_results[0]['score'] if final_results else 0})")
            return final_results
            
        except ImportError:
            logger.warning("Rerank module import failed, falling back to basic sort.")
        except Exception as e:
            logger.error(f"Rerank process failed: {e}", exc_info=True)
            
        # Fallback: Sort by score descending
        final_results = sorted(combined.values(), key=lambda x: x["score"], reverse=True)[:top_k]
        return final_results
    
    def embed_document(
        self,
        doc_id: str,
        text: str,
        metadata: Dict[str, Any],
        user_id: int = 1
    ) -> bool:
        """
        ä¸ºæ–‡æ¡£ç”Ÿæˆå‘é‡å¹¶ç›´æ¥å†™å…¥ PostgreSQL
        :param doc_id: æ–‡æ¡£ IDï¼ˆå¯¹åº”æ•°æ®åº“è®°å½• IDï¼‰
        :param text: æ–‡æ¡£æ–‡æœ¬å†…å®¹
        :param metadata: å…ƒæ•°æ®ï¼ˆfilename, category ç­‰ï¼Œä¸å†ä½¿ç”¨ï¼‰
        :param user_id: ç”¨æˆ·IDï¼Œé¢„ç•™å¤šç”¨æˆ·æ‰©å±•
        :return: æ˜¯å¦æˆåŠŸ
        """
        if not self.db:
            logger.warning("æ•°æ®åº“ä¼šè¯æœªæä¾›ï¼Œæ— æ³•å†™å…¥å‘é‡")
            return False
        
        try:
            # 1. è°ƒç”¨ Embedding API ç”Ÿæˆå‘é‡
            from src.services.ai_service import AIService
            ai_service = AIService()
            vector = ai_service.embed_text(text, db_session=self.db)
            
            if not vector:
                logger.warning("âš ï¸ Embedding API è¿”å›ç©ºå‘é‡")
                return False
            
            # 2. ç›´æ¥å†™å…¥ PostgreSQL çš„ embedding å­—æ®µ
            from datetime import datetime
            record = self.db.query(ArchiveRecord).filter(
                ArchiveRecord.id == int(doc_id),
                ArchiveRecord.user_id == user_id
            ).first()
            
            if not record:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°è®°å½•: {doc_id}")
                return False
            
            # æ›´æ–°å‘é‡å­—æ®µ
            record.embedding = vector  # pgvector ä¼šè‡ªåŠ¨å¤„ç†å‘é‡ç±»å‹è½¬æ¢
            record.is_vectorized = 1
            record.vectorized_at = datetime.now()
            
            self.db.commit()
            logger.info(f"âœ… æ–‡æ¡£å‘é‡åŒ–æˆåŠŸå¹¶å†™å…¥æ•°æ®åº“: {doc_id}")
            return True
        except Exception as e:
            logger.error(f"âŒ æ–‡æ¡£å‘é‡åŒ–å¤±è´¥: {e}", exc_info=True)
            if self.db:
                self.db.rollback()
            return False
    
    def delete_document_vector(self, doc_id: str) -> bool:
        """ä»æ•°æ®åº“ä¸­åˆ é™¤æ–‡æ¡£å‘é‡ï¼ˆæ¸…ç©º embedding å­—æ®µï¼‰"""
        if not self.db:
            return False
        
        try:
            record = self.db.query(ArchiveRecord).filter(
                ArchiveRecord.id == int(doc_id)
            ).first()
            
            if record:
                record.embedding = None
                record.is_vectorized = 0
                record.vectorized_at = None
                self.db.commit()
                logger.info(f"âœ… æ–‡æ¡£å‘é‡å·²åˆ é™¤: {doc_id}")
                return True
            else:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°è®°å½•: {doc_id}")
                return False
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤æ–‡æ¡£å‘é‡å¤±è´¥: {e}")
            if self.db:
                self.db.rollback()
            return False